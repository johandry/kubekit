package ec2

// Code generated automatically by 'go run codegen/main.go --pkg <pkg> --src <pkg>/templates --dst <pkg>/code.go'; DO NOT EDIT THIS FILE.

func init() {
	ResourceTemplates = map[string]string{
		"data-sources": dataSourcesTpl,
		"output":       outputTpl,
		"provider":     providerTpl,
		"resources":    resourcesTpl,
		"variables":    variablesTpl,
	}
}

// Expressions in the templates
/**
data-sources : {{ $.AwsVpcID }}
data-sources : {{ $.AwsVpcID }}
data-sources : {{ range $k, $v := .NodePools }}
data-sources : {{ if gt $v.Count 0 }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ $v.Count }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ Dash ( Lower $.ClusterName ) }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ Dash ( Lower $.ClusterName ) }}
data-sources : {{ Dash ( Lower $v.Name ) }}
data-sources : {{ end }}
data-sources : {{ end }}
output : {{ $masterNodePool := MasterPool $.NodePools }}
output : {{- range $k, $v := $.NodePools -}}
output : {{- range $i := Count $v.Count  }}
output : {{- Dash $v.Name }}
output : {{ $i }}
output : {{- Dash $v.Name }}
output : {{ $i }}
output : {{- Dash $v.Name }}
output : {{ $i }}
output : {{- Dash $v.Name }}
output : {{ $i }}
output : {{ Dash ( Lower $v.Name ) }}
output : {{ Dash ( Lower $v.Name ) }}
output : {{ end }}
output : {{ end }}
output : {{- $first := true -}}
output : {{- range $k, $v := $.ElasticFileshares -}}
output : {{- if $first -}}
output : {{- $first = false -}}
output : {{- else -}}
output : {{end -}}
output : {{- Dash ( Lower $k ) }}
output : {{- Dash ( Lower $k ) }}
output : {{- Dash ( Lower $k ) }}
output : {{- end }}
resources : {{ $masterNodePool := MasterPool $.NodePools }}
resources : {{ range $k, $v := .NodePools }}
resources : {{ if and $v.PGStrategy (isPGStrategy $v.PGStrategy) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ $v.PGStrategy }}
resources : {{- end }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name )  }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ $v.Ami }}
resources : {{ $v.InstanceType }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ QuoteList $v.SecurityGroups }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ $v.RootVolumeSize -}}
resources : {{ $v.RootVolumeType -}}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ if and $v.PGStrategy (isPGStrategy $v.PGStrategy) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{- end }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ $v.Count }}
resources : {{ $v.Count }}
resources : {{ $v.Count }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ if $v.PGStrategy }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{- end }}
resources : {{ QuoteList $v.Subnets }}
resources : {{ if eq $v.Name $masterNodePool.Name -}}
resources : {{- end }}
resources : {{ Dash ( Lower $k ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{- Dash $v.Name }}
resources : {{ $v.Count }}
resources : {{ $v.ConnectionTimeout }}
resources : {{ $.Username }}
resources : {{- if $.ConfigureFromPrivateNet -}}
resources : {{- Dash $v.Name }}
resources : {{- else -}}
resources : {{- Dash $v.Name }}
resources : {{- end }}
resources : {{ end }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ ( QuoteList ( AllSubNets ) ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ $.KubeAPISSLPort }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ $.KubeVIPAPISSLPort }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ $.KubeAPISSLPort }}
resources : {{ $.KubeVIPAPISSLPort }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Trim .PublicKey }}
resources : {{ range $k, $v := .NodePools }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $.ClusterName ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ Dash ( Lower $v.Name ) }}
resources : {{ if eq $v.Name $masterNodePool.Name }}
resources : {{ else }}
resources : {{ end }}
resources : {{ end }}
resources : {{ range $k, $v := .ElasticFileshares }}
resources : {{ Dash ( Lower $k  ) }}
resources : {{ Dash $.ClusterName }}
resources : {{ Dash $k }}
resources : {{ $v.PerformanceMode }}
resources : {{ $v.ThroughputMode }}
resources : {{ $v.Encrypted }}
resources : {{ Dash $.ClusterName }}
resources : {{ Dash ( Lower $k ) }}
resources : {{ range $s := AllSubNets }}
resources : {{ Dash $.ClusterName }}
resources : {{ Dash ( Lower $k ) }}
resources : {{ $s }}
resources : {{ Dash ( Lower $k ) }}
resources : {{ Dash ( Lower $k ) }}
resources : {{ $s }}
resources : {{ QuoteList AllSecGroups }}
resources : {{ end }}
resources : {{ end }}
variables : {{ range $k, $v := .NodePools }}
variables : {{ Dash ( Lower $v.Name ) }}
variables : {{- if IsFastEphemeral $v }}
variables : {{- end }}
variables : {{ end }}
**/

const dataSourcesTpl = `# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# data_sources.tf collects data and set's variables to be used later.  
# It does nothing to modify the images

data "aws_region" "current" {}

data "aws_vpc" "vpc" {
  id            = "{{ $.AwsVpcID }}"
}

data "aws_subnet_ids" "vpc_subnet" {
  vpc_id        = "{{ $.AwsVpcID }}"
}

data "aws_caller_identity" "current" {}

{{ range $k, $v := .NodePools }}

  {{ if gt $v.Count 0 }}
data "aws_instance" "{{ Dash ( Lower $v.Name ) }}" {
  count = "{{ $v.Count }}"
  depends_on = ["data.aws_instances.{{ Dash ( Lower $v.Name ) }}"]
  instance_id = data.aws_instances.{{ Dash ( Lower $v.Name ) }}.ids[count.index]
}
  

data "aws_instances" "{{ Dash ( Lower $v.Name ) }}" {
  depends_on = [ "aws_autoscaling_group.{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}",
  ]
  instance_tags = {
    Name = "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}" 
  }
}
  {{ end }}
{{ end }}`

const outputTpl = `{{ $masterNodePool := MasterPool $.NodePools }}

output "service_ip" {
  value = aws_alb.alb.dns_name
}

output "service_port" {
  value = aws_alb_listener.kube-vip-api-ssl-port.port
}

output "alb_dns" {
  value = aws_alb.alb.dns_name
}

output "kube_vip_api_ssl_port" {
  value = aws_alb_listener.kube-vip-api-ssl-port.port
}

output "kube_api_ssl_port" {
  value = aws_alb_listener.kube-api-ssl-port.port
}

output "nodes" {
  value = [ {{- range $k, $v := $.NodePools -}} {{- range $i := Count $v.Count  }}
      "{\"private_ip\": \"${data.aws_instance.
      {{- Dash $v.Name }}.{{ $i }}.private_ip}\",\"public_ip\": \"${data.aws_instance.
      {{- Dash $v.Name }}.{{ $i }}.public_ip}\",\"public_dns\": \"${data.aws_instance.
      {{- Dash $v.Name }}.{{ $i }}.public_dns}\",\"private_dns\": \"${data.aws_instance.
      {{- Dash $v.Name }}.{{ $i }}.private_dns}\",\"role\": \"{{ Dash ( Lower $v.Name ) }}\",\"pool\": \"{{ Dash ( Lower $v.Name ) }}\"}",{{ end }}{{ end }}
  ]
}

output "elastic-fileshares" {
  value = "[ {{- $first := true -}}{{- range $k, $v := $.ElasticFileshares -}}
    {{- if $first -}}{{- $first = false -}}{{- else -}}, {{end -}}
    { \"efs_name\": \"{{- Dash ( Lower $k ) }}\",\"efs_id\": \"${aws_efs_file_system.efs-
    {{- Dash ( Lower $k ) }}.id}\",\"efs_dns\": \"${aws_efs_file_system.efs-
    {{- Dash ( Lower $k ) }}.dns_name}\",\"efs_region\": \"${data.aws_region.current.name}\" }
    {{- end }} ]"
}`

const providerTpl = `# Provider 
# ==============================================================================
provider "aws" {
  access_key = var.aws_access_key
  secret_key = var.aws_secret_key
  region     = var.aws_region
  token      = var.aws_token
}
`

const resourcesTpl = `# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# resources.tf collects creates the resources that will be used with the image.  
# Be careful with what you create as a resource, as you can overwrite existing 
# infrastructure easily.

# AWS Instances
# ==============================================================================
{{ $masterNodePool := MasterPool $.NodePools }}

{{ range $k, $v := .NodePools }}
  {{ if and $v.PGStrategy (isPGStrategy $v.PGStrategy) }} 

resource "aws_placement_group" "{{ Dash ( Lower $.ClusterName ) }}-node-pool-{{ Dash ( Lower $v.Name ) }}" {
  name     = "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}"

  strategy = "{{ $v.PGStrategy }}"
}
  {{- end }}

resource "aws_launch_configuration" "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}" {
  depends_on = ["aws_iam_instance_profile.kube-{{ Dash ( Lower $v.Name )  }}-profile"]
  associate_public_ip_address = true
  ebs_optimized               = true
  iam_instance_profile        = "{{ Dash ( Lower $.ClusterName ) }}-{{ Dash ( Lower $v.Name ) }}-profile"
  image_id                    = "{{ $v.Ami }}"
  instance_type               = "{{ $v.InstanceType }}"
  name_prefix                 = "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}-"
  security_groups             = [{{ QuoteList $v.SecurityGroups }}]
  user_data_base64            = base64encode(local.node-{{ Dash ( Lower $v.Name ) }}-userdata)
  key_name                    = "{{ Dash ( Lower $.ClusterName ) }}-key"

  root_block_device {
    delete_on_termination = true
    volume_size           = "{{ $v.RootVolumeSize -}}"
    volume_type           = "{{ $v.RootVolumeType -}}"
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_autoscaling_group" "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}" {
  depends_on           = [ 
      {{ if and $v.PGStrategy (isPGStrategy $v.PGStrategy) }} 
    "aws_placement_group.{{ Dash ( Lower $.ClusterName ) }}-node-pool-{{ Dash ( Lower $v.Name ) }}",
      {{- end }}
    "aws_launch_configuration.{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}" 
  ]
  name                 = "{{ Dash ( Lower $.ClusterName ) }}-node-pool-{{ Dash ( Lower $v.Name ) }}"
  desired_capacity     = "{{ $v.Count }}"
  max_size             = "{{ $v.Count }}"
  min_size             = "{{ $v.Count }}"
  launch_configuration = aws_launch_configuration.{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}.name
  {{ if $v.PGStrategy }}
  placement_group      = "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}"
  {{- end }}
  vpc_zone_identifier  = [ {{ QuoteList $v.Subnets }} ]
  {{ if eq $v.Name $masterNodePool.Name -}}
  target_group_arns    = [
    aws_alb_target_group.tgt-grp-api.arn,
    aws_alb_target_group.tgt-grp-vip.arn,
    //aws_alb_target_group.tgt-grp-ssh.arn,
  ]
  {{- end }}
  tag {
    key                 = "NodePool"
    value               = "{{ Dash ( Lower $k ) }}"
    propagate_at_launch = true
  }

  tag {
    key                 = "Name"
    value               = "{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}" // needs to be just "-worker" on 1.0
    propagate_at_launch = true
  }
 
  tag {
    key                 = "ClusterName"
    value               = "{{ Dash ( Lower $.ClusterName ) }}"
    propagate_at_launch = true
  }

  tag {
    key                 = "Project"
    value               = "KubeKit"
    propagate_at_launch = true
  }

  tag {
    key                 = "kubernetes.io/cluster/{{ Dash ( Lower $.ClusterName ) }}"
    value               = "owned"
    propagate_at_launch = true
  }

  # todo volume tags
  # https://github.com/terraform-providers/terraform-provider-aws/issues/9448

  lifecycle {
    create_before_destroy = true
  }
}


resource "null_resource" "wait-{{ Dash ( Lower $v.Name ) }}" {
  depends_on = [
    "aws_autoscaling_group.{{ Dash ( Lower $.ClusterName ) }}-node-{{ Dash ( Lower $v.Name ) }}",
    "data.aws_instance.{{- Dash $v.Name }}" 
  ]

  count       = "{{ $v.Count }}"

  connection {
    timeout     = "{{ $v.ConnectionTimeout }}"
    user        = "{{ $.Username }}"
    private_key = "${var.private_key}"
    host        =
      {{- if $.ConfigureFromPrivateNet -}}
        element( data.aws_instance.{{- Dash $v.Name }}.*.private_ip, count.index )
      {{- else -}}
        element( data.aws_instance.{{- Dash $v.Name }}.*.public_ip, count.index )
      {{- end }}
  }

  provisioner "file" {
    content      = "terraform was able to ssh to the instance"
    destination = "/tmp/terraform.up"
  }
}

{{ end }}

# AWS Load Balancers
# ==============================================================================

resource "aws_alb" "alb" {
  name    = "{{ Dash ( Lower $.ClusterName ) }}"
  subnets = [{{ ( QuoteList ( AllSubNets ) ) }}]
  internal                   = false
  load_balancer_type         = "network"
  enable_deletion_protection = false

  tags = {
    Cluster = "{{ Dash ( Lower $.ClusterName ) }}"
    Owner   = data.aws_caller_identity.current.arn
  }
}

resource "aws_alb_target_group" "tgt-grp-api" {
  depends_on = ["aws_alb.alb"]
  name       = "{{ Dash ( Lower $.ClusterName ) }}-api"
  port       = "{{ $.KubeAPISSLPort }}"
  protocol   = "TCP"
  vpc_id     = data.aws_vpc.vpc.id

  tags = {
    Cluster = "{{ Dash ( Lower $.ClusterName ) }}"
    Owner   = data.aws_caller_identity.current.arn
  }
}

resource "aws_alb_target_group" "tgt-grp-vip" {
  depends_on = ["aws_alb.alb"]
  name       = "{{ Dash ( Lower $.ClusterName ) }}-vip"
  port       = "{{ $.KubeVIPAPISSLPort }}"
  protocol   = "TCP"
  vpc_id     = data.aws_vpc.vpc.id

  tags = {
    Cluster = "{{ Dash ( Lower $.ClusterName ) }}"
    Owner   = data.aws_caller_identity.current.arn
  }
}

# resource "aws_alb_target_group" "tgt-grp-ssh" {
#   depends_on = ["aws_alb.alb"]
#   name       = "{{ Dash ( Lower $.ClusterName ) }}-ssh"
#   port       = 22
#   protocol   = "TCP"
#   vpc_id     = data.aws_vpc.vpc.id

#   tags = {
#     Cluster = "{{ Dash ( Lower $.ClusterName ) }}"
#     Owner   = data.aws_caller_identity.current.arn
#   }
# }

resource "aws_alb_listener" "kube-api-ssl-port" {
  depends_on        = ["aws_alb.alb"]
  load_balancer_arn = "${aws_alb.alb.arn}"
  port              = "{{ $.KubeAPISSLPort }}"
  protocol          = "TCP"

  default_action {
    target_group_arn = aws_alb_target_group.tgt-grp-api.arn
    type             = "forward"
  }
}

resource "aws_alb_listener" "kube-vip-api-ssl-port" {
  depends_on        = ["aws_alb.alb"]
  load_balancer_arn = aws_alb.alb.arn
  port              = "{{ $.KubeVIPAPISSLPort }}"
  protocol          = "TCP"

  default_action {
    target_group_arn = aws_alb_target_group.tgt-grp-api.arn
    type             = "forward"
  }
}

# resource "aws_alb_listener" "ssh-listener" {
#   depends_on        = ["aws_alb.alb"]
#   load_balancer_arn = aws_alb.alb.arn
#   port              = 22
#   protocol          = "TCP"

#   default_action {
#     target_group_arn = aws_alb_target_group.tgt-grp-ssh.arn
#     type             = "forward"
#   }
# }

# AWS IAM
# ==============================================================================



resource "aws_key_pair" "keypair" {
  // TODO need to verify if key name change will cause destruction on existing 1.0 systems
  key_name   = "{{ Dash ( Lower $.ClusterName ) }}-key"
  public_key = "{{ Trim .PublicKey }}"
}

{{ range $k, $v := .NodePools }}

resource "aws_iam_instance_profile" "kube-{{ Dash ( Lower $v.Name ) }}-profile" {
  depends_on = ["aws_iam_role.kube-{{ Dash ( Lower $v.Name ) }}-role"]
  name       = "{{ Dash ( Lower $.ClusterName ) }}-{{ Dash ( Lower $v.Name ) }}-profile"
  role       = aws_iam_role.kube-{{ Dash ( Lower $v.Name ) }}-role.name
}

resource "aws_iam_role" "kube-{{ Dash ( Lower $v.Name ) }}-role" {
  name = "{{ Dash ( Lower $.ClusterName ) }}-{{ Dash ( Lower $v.Name ) }}-role"

  assume_role_policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": { "Service": "ec2.amazonaws.com" },
      "Effect": "Allow"
    }
  ]
}
EOF
}

resource "aws_iam_role_policy" "kube-{{ Dash ( Lower $v.Name ) }}-policy" {
  name       = "{{ Dash ( Lower $.ClusterName ) }}-{{ Dash ( Lower $v.Name ) }}-policy"
  role       = aws_iam_role.kube-{{ Dash ( Lower $v.Name ) }}-role.id
  depends_on = ["aws_iam_role.kube-{{ Dash ( Lower $v.Name ) }}-role"]

  policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {{ if eq $v.Name $masterNodePool.Name }}
    {
      "Effect": "Allow",
      "Action": [ "ec2:*" ],
      "Resource": [ "*" ]
    },
    {
      "Effect": "Allow",
      "Action": [ "elasticloadbalancing:*" ],
      "Resource": [ "*" ]
    },
    {{ else }}
    {
      "Effect": "Allow",
      "Action": "ec2:Describe*",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": "ec2:AttachVolume",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": "ec2:DetachVolume",
      "Resource": "*"
    },
    {{ end }}
    {
      "Effect": "Allow",
      "Action": [ "route53:*" ],
      "Resource": [ "*" ]
    },
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [ "arn:aws:s3:::kubernetes-*" ]
    }
  ]
}
EOF
}

{{ end }}


{{ range $k, $v := .ElasticFileshares }}
resource "aws_efs_file_system" "efs-{{ Dash ( Lower $k  ) }}" {
  creation_token = "{{ Dash $.ClusterName }}-efs-{{ Dash $k }}"
  
  performance_mode = "{{ $v.PerformanceMode }}"
  throughput_mode = "{{ $v.ThroughputMode }}"
  encrypted = "{{ $v.Encrypted }}"

  tags = {
    Name = "{{ Dash $.ClusterName }}-efs-{{ Dash ( Lower $k ) }}"
  }
}

  {{ range $s := AllSubNets }}
resource "aws_efs_mount_target" "{{ Dash $.ClusterName }}-efs-{{ Dash ( Lower $k ) }}-{{ $s }}-mount" {
  depends_on = ["aws_efs_file_system.efs-{{ Dash ( Lower $k ) }}"]
  file_system_id = aws_efs_file_system.efs-{{ Dash ( Lower $k ) }}.id
  subnet_id      = "{{ $s }}"
  security_groups = [ {{ QuoteList AllSecGroups }} ]
}
  {{ end }}
{{ end }}`

const variablesTpl = `variable "aws_access_key" {}
variable "aws_secret_key" {}
variable "aws_region"   {}
variable "aws_token"   {}
variable "private_key"   {}

locals {

{{ range $k, $v := .NodePools }}
  node-{{ Dash ( Lower $v.Name ) }}-userdata =  <<USERDATA
#!/bin/bash
ulimit -n 65535
cat <<EOF > /etc/security/limits.d/30-nofile.conf
root       soft    nofile     65536
root       hard    nofile     65536
EOF
cat <<EOF > /etc/sysctl.d/99-kubelet-network.conf
# Have a larger connection range available
net.ipv4.ip_local_port_range=1024 65000

# Reuse closed sockets faster
net.ipv4.tcp_tw_reuse=1
net.ipv4.tcp_fin_timeout=15

# The maximum number of "backlogged sockets".  Default is 128.
net.core.somaxconn=4096
net.core.netdev_max_backlog=4096

# 16MB per socket - which sounds like a lot,
# but will virtually never consume that much.
net.core.rmem_max=16777216
net.core.wmem_max=16777216

# Various network tunables
net.ipv4.tcp_max_syn_backlog=20480
net.ipv4.tcp_max_tw_buckets=400000
net.ipv4.tcp_no_metrics_save=1
net.ipv4.tcp_rmem=4096 87380 16777216
net.ipv4.tcp_syn_retries=2
net.ipv4.tcp_synack_retries=2
net.ipv4.tcp_wmem=4096 65536 16777216
#vm.min_free_kbytes=65536

# Connection tracking to prevent dropped connections (usually issue on LBs)
net.netfilter.nf_conntrack_max=262144
net.ipv4.netfilter.ip_conntrack_generic_timeout=120
net.netfilter.nf_conntrack_tcp_timeout_established=86400

# ARP cache settings for a highly loaded docker swarm
net.ipv4.neigh.default.gc_thresh1=8096
net.ipv4.neigh.default.gc_thresh2=12288
net.ipv4.neigh.default.gc_thresh3=16384

# disable ipv6
net.ipv6.conf.all.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6=1
net.ipv6.conf.lo.disable_ipv6=1

# set max_map_count for elasticsearch usage
# vm.max_map_count= would be -> MaxMapCount 

fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=524288
EOF
systemctl restart systemd-sysctl.service
set -o xtrace
  {{- if IsFastEphemeral $v }}
pvcreate /dev/nvme[1-9]*n*
vgcreate vgdata /dev/nvme[1-9]*n*
lvcreate -l 100%FREE --type striped -n lvoldata vgdata
mkfs.xfs /dev/vgdata/lvoldata
mkdir -p /data
mount /dev/vgdata/lvoldata /data
echo "/dev/vgdata/lvoldata /data xfs defaults 0 0"  >> /etc/fstab
for directory in /data/docker /data/kubelet; do
  if [ ! -d "$${directory}" ]; then
    mkdir -p $${directory}
  fi
done
for directory in /var/lib/docker /var/lib/kubelet; do
  if [ ! -d "$${directory}" ]; then
    mkdir -p $${directory}
  fi
  rm -Rf $${directory}/*
  mount --bind /data/$(basename $${directory}) $${directory}
  echo "$${directory} /data/$(basename $${directory}) bind bind 0 0"  >> /etc/fstab
done
  {{- end }}

# pull needed config shit from s3 and init

USERDATA

{{ end }}
}`
