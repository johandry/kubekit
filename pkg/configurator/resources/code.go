package resources

// Code generated automatically by 'go run ../codegen/kubernetes/main.go --src ../templates/resources --dst code.go'; DO NOT EDIT THIS FILE.

func init() {
	ResourceTemplates = map[string]string{
		"aks-aad-pod-identity-nmi-mic": aksAadPodIdentityNmiMicTpl,
		"aks-acr-docker-secret":        aksAcrDockerSecretTpl,
		"aks-network-policies":         aksNetworkPoliciesTpl,
		"audit-policy":                 auditPolicyTpl,
		"aws-auth":                     awsAuthTpl,
		"azure-storage-classes":        azureStorageClassesTpl,
		"backend-policy":               backendPolicyTpl,
		"ceph-critical":                cephCriticalTpl,
		"control-plane-services":       controlPlaneServicesTpl,
		"default-deny":                 defaultDenyTpl,
		"default":                      defaultTpl,
		"dns-policy":                   dnsPolicyTpl,
		"ebs-blockstore":               ebsBlockstoreTpl,
		"efs-filestore":                efsFilestoreTpl,
		"eks-calico":                   eksCalicoTpl,
		"eks-heapster":                 eksHeapsterTpl,
		"eks-network-policies":         eksNetworkPoliciesTpl,
		"eventratelimit":               eventratelimitTpl,
		"frontend-policy":              frontendPolicyTpl,
		"heapster-policy":              heapsterPolicyTpl,
		"kube-state-metrics-policy":    kubeStateMetricsPolicyTpl,
		"kube-state-metrics":           kubeStateMetricsTpl,
		"kube-system-critical":         kubeSystemCriticalTpl,
		"kube-system-high":             kubeSystemHighTpl,
		"kube-system":                  kubeSystemTpl,
		"open-policy-agent":            openPolicyAgentTpl,
		"pod-security-policies":        podSecurityPoliciesTpl,
		"priority-classes":             priorityClassesTpl,
		"privileged-psp":               privilegedPspTpl,
		"privileged-rbac":              privilegedRbacTpl,
		"prometheus-service":           prometheusServiceTpl,
		"prometheus":                   prometheusTpl,
		"resource-quotas":              resourceQuotasTpl,
		"restricted-psp":               restrictedPspTpl,
		"restricted-rbac":              restrictedRbacTpl,
		"rook-blockstore":              rookBlockstoreTpl,
		"rook-ceph-system":             rookCephSystemTpl,
		"rook-ceph":                    rookCephTpl,
		"rook-cluster":                 rookClusterTpl,
		"rook-common":                  rookCommonTpl,
		"rook-filestore":               rookFilestoreTpl,
		"rook-object-user":             rookObjectUserTpl,
		"rook-objectstore":             rookObjectstoreTpl,
		"rook-operator":                rookOperatorTpl,
		"rook-storage-class":           rookStorageClassTpl,
		"rook-toolbox":                 rookToolboxTpl,
		"service-monitor":              serviceMonitorTpl,
		"vsphere-volumes":              vsphereVolumesTpl,
	}
}

// Expressions in the templates
/**
aks-acr-docker-secret : {{ .AzureACRDockerConfigJsonBase64 }}
aws-auth : {{ .roleARN }}
efs-filestore : {{- range $share := unmarshallEFS $.elasticFileshares }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.ID }}
efs-filestore : {{ $share.Region }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.DNS }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.ID }}
efs-filestore : {{ $share.Region }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.ID }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{ $share.Name }}
efs-filestore : {{- end }}
eks-heapster : {{ .heapsterImageSrc }}
eks-heapster : {{ .heapsterImageSrc }}
eks-heapster : {{ .addonResizerImageSrc }}
eks-heapster : {{ .heapsterNannyMemory }}
eks-heapster : {{ .heapsterNannyMemory }}
eks-heapster : {{ .addonResizerImageSrc }}
eks-heapster : {{ .heapsterNannyMemory }}
eks-heapster : {{ .heapsterNannyMemory }}
open-policy-agent : {{ publicKey .certsPath .platform "opa" | base64Encode }}
open-policy-agent : {{ privateKey .certsPath .platform "opa" | base64Encode }}
rook-objectstore : {{ cert .certsPath .platform "ingress" }}
rook-objectstore : {{ publicKey .certsPath .platform "ingress_root_ca" | base64Encode }}
vsphere-volumes : {{ .vsphereServer }}
vsphere-volumes : {{ base64Encode .vsphereUsername }}
vsphere-volumes : {{ .vsphereServer }}
vsphere-volumes : {{ base64Encode .vspherePassword }}
**/

const aksAadPodIdentityNmiMicTpl = `# https://raw.githubusercontent.com/Azure/aad-pod-identity/master/deploy/infra/deployment-rbac.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: aad-pod-id-nmi-service-account
  namespace: kube-system
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: azureassignedidentities.aadpodidentity.k8s.io
spec:
  group: aadpodidentity.k8s.io
  version: v1
  names:
    kind: AzureAssignedIdentity
    plural: azureassignedidentities
  scope: Namespaced
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: azureidentitybindings.aadpodidentity.k8s.io
spec:
  group: aadpodidentity.k8s.io
  version: v1
  names:
    kind: AzureIdentityBinding
    plural: azureidentitybindings
  scope: Namespaced
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: azureidentities.aadpodidentity.k8s.io
spec:
  group: aadpodidentity.k8s.io
  version: v1
  names:
    kind: AzureIdentity
    singular: azureidentity
    plural: azureidentities
  scope: Namespaced
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: azurepodidentityexceptions.aadpodidentity.k8s.io
spec:
  group: aadpodidentity.k8s.io
  version: v1
  names:
    kind: AzurePodIdentityException
    singular: azurepodidentityexception
    plural: azurepodidentityexceptions
  scope: Namespaced
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aad-pod-id-nmi-role
rules:
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get"]
  - apiGroups: ["aadpodidentity.k8s.io"]
    resources: ["azureidentitybindings", "azureidentities", "azurepodidentityexceptions"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["aadpodidentity.k8s.io"]
    resources: ["azureassignedidentities"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aad-pod-id-nmi-binding
  labels:
    k8s-app: aad-pod-id-nmi-binding
subjects:
  - kind: ServiceAccount
    name: aad-pod-id-nmi-service-account
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: aad-pod-id-nmi-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    kubernetes.io/cluster-service: "true"
    component: nmi
    tier: node
    k8s-app: aad-pod-id
  name: nmi
  namespace: kube-system
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      component: nmi
      tier: node
  template:
    metadata:
      labels:
        component: nmi
        tier: node
    spec:
      serviceAccountName: aad-pod-id-nmi-service-account
      hostNetwork: true
      volumes:
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: iptableslock
      containers:
        - name: nmi
          image: "mcr.microsoft.com/k8s/aad-pod-identity/nmi:1.5.3"
          imagePullPolicy: Always
          args:
            - "--host-ip=$(HOST_IP)"
            - "--node=$(NODE_NAME)"
          env:
            - name: HOST_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          securityContext:
            privileged: true
            capabilities:
              add:
                - NET_ADMIN
          volumeMounts:
            - mountPath: /run/xtables.lock
              name: iptableslock
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
      nodeSelector:
        beta.kubernetes.io/os: linux
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aad-pod-id-mic-service-account
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aad-pod-id-mic-role
rules:
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["*"]
  - apiGroups: [""]
    resources: ["pods", "nodes"]
    verbs: [ "list", "watch" ]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["create", "get","update"]
  - apiGroups: ["aadpodidentity.k8s.io"]
    resources: ["azureidentitybindings", "azureidentities"]
    verbs: ["get", "list", "watch", "post"]
  - apiGroups: ["aadpodidentity.k8s.io"]
    resources: ["azureassignedidentities"]
    verbs: ["*"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aad-pod-id-mic-binding
  labels:
    k8s-app: aad-pod-id-mic-binding
subjects:
  - kind: ServiceAccount
    name: aad-pod-id-mic-service-account
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: aad-pod-id-mic-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: mic
    k8s-app: aad-pod-id
  name: mic
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      component: mic
      app: mic
  template:
    metadata:
      labels:
        component: mic
        app: mic
    spec:
      serviceAccountName: aad-pod-id-mic-service-account
      containers:
        - name: mic
          image: "mcr.microsoft.com/k8s/aad-pod-identity/mic:1.5.3"
          imagePullPolicy: Always
          args:
            - "--cloudconfig=/etc/kubernetes/azure.json"
            - "--logtostderr"
          resources:
            limits:
              cpu: 200m
              memory: 1024Mi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
            - name: k8s-azure-file
              mountPath: /etc/kubernetes/azure.json
              readOnly: true
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
      volumes:
        - name: k8s-azure-file
          hostPath:
            path: /etc/kubernetes/azure.json
      nodeSelector:
        beta.kubernetes.io/os: linux
`

const aksAcrDockerSecretTpl = `apiVersion: v1
kind: Secret
metadata:
  name: acr-read
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: "{{ .AzureACRDockerConfigJsonBase64 }}"
`

const aksNetworkPoliciesTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
    - from:
        - podSelector:
            matchLabels:
              role: frontend

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: dns-policy
  namespace: kube-system
spec:
  ingress:
    - ports:
        - port: 53
          protocol: UDP
        - port: 53
          protocol: TCP
  podSelector:
    matchLabels:
      k8s-app: kube-dns
  policyTypes:
    - Ingress

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
    - ports:
        # Allow http traffic to frontend pods
        - port: 80
        - port: 443
        - port: 30080
        - port: 30443
        - port: 5000
        - port: 3000
        - port: 5601
        - port: 8080
        - port: 8081
        - port: 8082
        - port: 8083
        - port: 9089
        - port: 9090
        - port: 9093
        - port: 8443
        - port: 5193
      from: []

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: kube-state-metrics-policy
spec:
  podSelector:
    matchLabels:
      app: kube-state-metrics
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kube-state-metrics: ingress
        - podSelector:
            matchLabels:
              app: kube-state-metrics
        - podSelector:
            matchLabels:
              kube-state-metrics: enabled

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny
  namespace: kube-system
spec:
  podSelector: {}
`

const auditPolicyTpl = `---
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
`

const awsAuthTpl = `---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: {{ .roleARN }}
      username: system:node:{{"{{EC2PrivateDNSName}}"}}
      groups:
        - system:bootstrappers
        - system:nodes
`

const azureStorageClassesTpl = `---
# -------------------
# AZURE BLOCK STORAGE
# -------------------
apiVersion: v1
kind: List
items:

  # Persistent volume claims are specified in GiB but Azure managed disks are
  # billed by SKU for a specific size. These SKUs range from 32GiB for S4 or 
  # P4 disks to 32TiB for S80 or P80 disks. 
  #
  # The throughput and IOPS performance of a Premium managed disk depends on the
  # both the SKU and the instance size of the nodes in the AKS cluster. 
  #
  # For more information, see Pricing and Performance of Managed Disks 
  # (https://azure.microsoft.com/pricing/details/managed-disks/)

  # DETAILS https://docs.microsoft.com/en-us/azure/aks/azure-disks-dynamic-pv

  # ------------------------------
  # The default storage class provisions a standard Azure disk.
  # Standard storage is backed by HDDs, and delivers cost-effective
  # storage while still being performant. Standard disks are ideal for 
  # a cost effective dev and test workload.

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: azure-standard-block-delete
    provisioner: kubernetes.io/azure-disk
    parameters:
      cachingmode: ReadOnly
      kind: Managed
      storageaccounttype: Standard_LRS
    reclaimPolicy: Delete
    # volumeBindingMode: Immediate / WaitForFirstConsumer

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: azure-standard-block-retain
    provisioner: kubernetes.io/azure-disk
    parameters:
      cachingmode: ReadOnly
      kind: Managed
      storageaccounttype: Standard_LRS
    reclaimPolicy: Retain
    # volumeBindingMode: Immediate / WaitForFirstConsumer

  # ------------------------------
  # The managed-premium storage class provisions a premium Azure disk.
  # Premium disks are backed by SSD-based high-performance, low-latency disk. 
  # Perfect for VMs running production workload. If the AKS nodes in your
  # cluster use premium storage, select the managed-premium class.

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: azure-premium-block-delete
    provisioner: kubernetes.io/azure-disk
    parameters:
      cachingmode: ReadOnly
      kind: Managed
      storageaccounttype: Premium_LRS
    reclaimPolicy: Delete
    # volumeBindingMode: Immediate / WaitForFirstConsumer

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: azure-premium-block-retain
    provisioner: kubernetes.io/azure-disk
    parameters:
      cachingmode: ReadOnly
      kind: Managed
      storageaccounttype: Premium_LRS
    reclaimPolicy: Retain
    # volumeBindingMode: Immediate / WaitForFirstConsumer


---
# ------------------
# AZURE FILE STORAGE
# ------------------
apiVersion: v1
kind: List
items:

  # Azure Files currently only work with Standard storage. 
  # If you use Premium storage, the volume fails to provision.

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: azure-standard-file
    provisioner: kubernetes.io/azure-file
    mountOptions:
      - dir_mode=0777
      - file_mode=0777
      - uid=1000
      - gid=1000
    parameters:
      skuName: Standard_LRS

  # RBAC Roles requied for Azure File Store Operation

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: system:azure-cloud-provider
    rules:
      - apiGroups: ['']
        resources: ['secrets']
        verbs:     ['get','create']

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: system:azure-cloud-provider
    roleRef:
      kind: ClusterRole
      apiGroup: rbac.authorization.k8s.io
      name: system:azure-cloud-provider
    subjects:
      - kind: ServiceAccount
        name: persistent-volume-binder
        namespace: kube-system
`

const backendPolicyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
      - from:
        - podSelector:
            matchLabels:
              role: frontend
          
        
`

const cephCriticalTpl = `---
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: ceph-critical
value: 900000000
globalDefault: false
description: |
  Critical pods that are not considered system level and reside in the rook-ceph-system namespace.
  This will still get trumped by the system level critical classes.
`

const controlPlaneServicesTpl = `---
# this can't be created in the static pod manifest and they are not needed by the control plane
# so we create the service in the core role since we only create them to be scraped by prometheus
# clusterIPs are set to None because they dont need to be proxied

kind: Service
apiVersion: v1
metadata:
  name: etcd
  namespace: kube-system
  labels:
    component: etcd
    tier: control-plane
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "2381"
spec:
  selector:
    component: etcd
    tier: control-plane
  ports:
  - name: metrics
    protocol: TCP
    port: 2381
  type: ClusterIP
  clusterIP: None
  sessionAffinity: None

---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-controller-manager
  labels:
    component: kube-controller-manager
    tier: control-plane
  annotations:
    prometheus.io/port: "10252"
    prometheus.io/scrape: "true"
spec:
  selector:
    component: kube-controller-manager
    tier: control-plane
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http-metrics
    port: 10252
    targetPort: 10252
    protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-scheduler
  labels:
    component: kube-scheduler
    tier: control-plane
  annotations:
    prometheus.io/port: "10251"
    prometheus.io/scrape: "true"
spec:
  selector:
    component: kube-scheduler
    tier: control-plane
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http-metrics
    port: 10251
    targetPort: 10251
    protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: kube-proxy
  labels:
    component: kube-proxy
    tier: control-plane
  annotations:
    prometheus.io/port: "10249"
    prometheus.io/scrape: "true"
spec:
  selector:
    component: kube-proxy
    tier: control-plane
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http-metrics
    port: 10249
    targetPort: 10249
    protocol: TCP
`

const defaultDenyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny
  namespace: kube-system
spec:
  podSelector: {}
`

const defaultTpl = `---
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
    name: default
value: 200000000
globalDefault: true
description: |
    Pods not assigned a PriorityClass will be dropped into the default class with value 200000000.
    Pods with lower priority are still possible`

const dnsPolicyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: dns-policy
  namespace: kube-system
spec:
  podSelector:
    matchLabels:
      app: kube-dns
  ingress:
  - {}
`

const ebsBlockstoreTpl = `---
apiVersion: v1
kind: List
items:

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-gp2-delete
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp2
    reclaimPolicy: Delete

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-gp2-retain
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp2
    reclaimPolicy: Retain

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-io1-delete
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: io1
      iopsPerGB: "50"
      fsType: ext4
    reclaimPolicy: Delete

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-io1-retain
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: io1
      iopsPerGB: "50"
      fsType: ext4
    reclaimPolicy: Retain

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-st1-delete
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: st1
      fsType: ext4
    reclaimPolicy: Delete

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-st1-retain
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: st1
      fsType: ext4
    reclaimPolicy: Retain

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-sc1-delete
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: sc1
      fsType: ext4
    reclaimPolicy: Delete

  - kind: StorageClass
    apiVersion: storage.k8s.io/v1
    metadata:
      name: ebs-sc1-retain
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: sc1
      fsType: ext4
    reclaimPolicy: Retain
`

const efsFilestoreTpl = `---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: kube-system
  name: efs-provisioner

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: efs-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-efs-provisioner
subjects:
  - kind: ServiceAccount
    name: efs-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: efs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-efs-provisioner
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-efs-provisioner
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: efs-provisioner
    namespace: kube-system
roleRef:
  kind: Role
  name: leader-locking-efs-provisioner
  apiGroup: rbac.authorization.k8s.io

{{- range $share := unmarshallEFS $.elasticFileshares }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "efs-provisioner-{{ $share.Name }}"
  namespace: kube-system
data:
  file.system.id: "{{ $share.ID }}"
  aws.region: "{{ $share.Region }}"
  provisioner.name: "kubekit.org/{{ $share.Name }}"
  dns.name: "{{ $share.DNS }}"

---
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: "efs-provisioner-{{ $share.Name }}"
  namespace: kube-system
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: "efs-provisioner-{{ $share.Name }}"
    spec:
      serviceAccount: efs-provisioner
      containers:
        - name: efs-provisioner
          image: quay.io/external_storage/efs-provisioner:v2.2.0-k8s1.12
          env:
            - name: FILE_SYSTEM_ID
              valueFrom:
                configMapKeyRef:
                  name: "efs-provisioner-{{ $share.Name }}"
                  key: file.system.id
            - name: AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: "efs-provisioner-{{ $share.Name }}"
                  key: aws.region
            - name: DNS_NAME
              valueFrom:
                configMapKeyRef:
                  name: "efs-provisioner-{{ $share.Name }}"
                  key: dns.name
                  optional: true
            - name: PROVISIONER_NAME
              valueFrom:
                configMapKeyRef:
                  name: "efs-provisioner-{{ $share.Name }}"
                  key: provisioner.name
          volumeMounts:
            - name: pv-volume
              mountPath: /persistentvolumes
      volumes:
        - name: pv-volume
          nfs:
            server: "{{ $share.ID }}.efs.{{ $share.Region }}.amazonaws.com"
            path: /
---

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: "efs-{{ $share.Name }}"
provisioner: kubekit.org/{{ $share.ID }}
---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: "efs-{{ $share.Name }}"
  annotations:
    volume.beta.kubernetes.io/storage-class: "efs-{{ $share.Name }}"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Mi
{{- end }}
`

const eksCalicoTpl = `---
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: calico-node
      annotations:
        # This, along with the CriticalAddonsOnly toleration below,
        # marks the pod as a critical add-on, ensuring it gets
        # priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      serviceAccountName: calico-node
      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
      # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
      terminationGracePeriodSeconds: 0
      containers:
        # Runs calico/node container on each Kubernetes node.  This
        # container programs network policy and routes on each
        # host.
        - name: calico-node
          image: quay.io/calico/node:v3.3.6
          env:
            # Use Kubernetes API as the backing datastore.
            - name: DATASTORE_TYPE
              value: "kubernetes"
            # Use eni not cali for interface prefix
            - name: FELIX_INTERFACEPREFIX
              value: "eni"
            # Enable felix info logging.
            - name: FELIX_LOGSEVERITYSCREEN
              value: "info"
            # Don't enable BGP.
            - name: CALICO_NETWORKING_BACKEND
              value: "none"
            # Cluster type to identify the deployment type
            - name: CLUSTER_TYPE
              value: "k8s,ecs"
            # Disable file logging so "kubectl logs" works.
            - name: CALICO_DISABLE_FILE_LOGGING
              value: "true"
            - name: FELIX_TYPHAK8SSERVICENAME
              value: "calico-typha"
            # Set Felix endpoint to host default action to ACCEPT.
            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
              value: "ACCEPT"
            # Disable IPV6 on Kubernetes.
            - name: FELIX_IPV6SUPPORT
              value: "false"
            # Wait for the datastore.
            - name: WAIT_FOR_DATASTORE
              value: "true"
            - name: FELIX_LOGSEVERITYSYS
              value: "none"
            - name: FELIX_PROMETHEUSMETRICSENABLED
              value: "true"
            - name: NO_DEFAULT_POOLS
              value: "true"
            # Set based on the k8s node name.
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # No IP address needed.
            - name: IP
              value: ""
            - name: FELIX_HEALTHENABLED
              value: "true"
          securityContext:
            privileged: true
          resources:
            requests:
              cpu: 250m
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9099
              host: localhost
            periodSeconds: 10
            initialDelaySeconds: 10
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/calico-node
                - -felix-ready
            periodSeconds: 10
          volumeMounts:
            - mountPath: /lib/modules
              name: lib-modules
              readOnly: true
            - mountPath: /run/xtables.lock
              name: xtables-lock
              readOnly: false
            - mountPath: /var/run/calico
              name: var-run-calico
              readOnly: false
            - mountPath: /var/lib/calico
              name: var-lib-calico
              readOnly: false
      volumes:
        # Used to ensure proper kmods are installed.
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        - name: var-lib-calico
          hostPath:
            path: /var/lib/calico
        - name: xtables-lock
          hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
      tolerations:
        # Make sure calico/node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists

---

# Create all the CustomResourceDefinitions needed for
# Calico policy-only mode.

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: felixconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: FelixConfiguration
    plural: felixconfigurations
    singular: felixconfiguration

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgpconfigurations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPConfiguration
    plural: bgpconfigurations
    singular: bgpconfiguration

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: ippools.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: IPPool
    plural: ippools
    singular: ippool

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: hostendpoints.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  versions:
    - name: v1
      served: true
      storage: true
  names:
    kind: HostEndpoint
    plural: hostendpoints
    singular: hostendpoint

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: clusterinformations.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  versions:
    - name: v1
      served: true
      storage: true
  names:
    kind: ClusterInformation
    plural: clusterinformations
    singular: clusterinformation

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworkpolicies.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  versions:
    - name: v1
      served: true
      storage: true
  names:
    kind: GlobalNetworkPolicy
    plural: globalnetworkpolicies
    singular: globalnetworkpolicy

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: globalnetworksets.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  versions:
    - name: v1
      served: true
      storage: true
  names:
    kind: GlobalNetworkSet
    plural: globalnetworksets
    singular: globalnetworkset

---

apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networkpolicies.crd.projectcalico.org
spec:
  scope: Namespaced
  group: crd.projectcalico.org
  versions:
    - name: v1
      served: true
      storage: true
  names:
    kind: NetworkPolicy
    plural: networkpolicies
    singular: networkpolicy

---

# Create the ServiceAccount and roles necessary for Calico.

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: calico-node
rules:
  - apiGroups: [""]
    resources:
      - namespaces
      - serviceaccounts
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - pods/status
    verbs:
      - patch
  - apiGroups: [""]
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups: [""]
    resources:
      - services
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - endpoints
    verbs:
      - get
  - apiGroups: [""]
    resources:
      - nodes
    verbs:
      - get
      - list
      - update
      - watch
  - apiGroups: ["extensions"]
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["networking.k8s.io"]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
    verbs:
      - create
      - get
      - list
      - update
      - watch

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: calico-node
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-node
subjects:
  - kind: ServiceAccount
    name: calico-node
    namespace: kube-system

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:
        k8s-app: calico-typha
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        cluster-autoscaler.kuberentes.io/safe-to-evict: 'true'
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
      hostNetwork: true
      serviceAccountName: calico-node
      containers:
        - image: quay.io/calico/typha:v3.3.6
          name: calico-typha
          ports:
            - containerPort: 5473
              name: calico-typha
              protocol: TCP
          env:
            # Use eni not cali for interface prefix
            - name: FELIX_INTERFACEPREFIX
              value: "eni"
            - name: TYPHA_LOGFILEPATH
              value: "none"
            - name: TYPHA_LOGSEVERITYSYS
              value: "none"
            - name: TYPHA_LOGSEVERITYSCREEN
              value: "info"
            - name: TYPHA_PROMETHEUSMETRICSENABLED
              value: "true"
            - name: TYPHA_CONNECTIONREBALANCINGMODE
              value: "kubernetes"
            - name: TYPHA_PROMETHEUSMETRICSPORT
              value: "9093"
            - name: TYPHA_DATASTORETYPE
              value: "kubernetes"
            - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
              value: "1"
            - name: TYPHA_HEALTHENABLED
              value: "true"
          livenessProbe:
            exec:
              command:
                - calico-typha
                - check
                - liveness
            periodSeconds: 30
            initialDelaySeconds: 30
          readinessProbe:
            exec:
              command:
                - calico-typha
                - check
                - readiness
            periodSeconds: 10

---

# This manifest creates a Pod Disruption Budget for Typha to allow K8s Cluster Autoscaler to evict
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: calico-typha

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: typha-cpha
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: typha-cpha
subjects:
  - kind: ServiceAccount
    name: typha-cpha
    namespace: kube-system

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: typha-cpha
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["list"]

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-typha-horizontal-autoscaler
  namespace: kube-system
data:
  ladder: |-
    {
      "coresToReplicas": [],
      "nodesToReplicas":
      [
        [1, 1],
        [10, 2],
        [100, 3],
        [250, 4],
        [500, 5],
        [1000, 6],
        [1500, 7],
        [2000, 8]
      ]
    }

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-typha-horizontal-autoscaler
  namespace: kube-system
  labels:
    k8s-app: calico-typha-autoscaler
spec:
  replicas: 1
  template:
    metadata:
      labels:
        k8s-app: calico-typha-autoscaler
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      containers:
        - image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2
          name: autoscaler
          command:
            - /cluster-proportional-autoscaler
            - --namespace=kube-system
            - --configmap=calico-typha-horizontal-autoscaler
            - --target=deployment/calico-typha
            - --logtostderr=true
            - --v=2
          resources:
            requests:
              cpu: 10m
            limits:
              cpu: 10m
      serviceAccountName: typha-cpha

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: typha-cpha
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources: ["deployments/scale"]
    verbs: ["get", "update"]

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: typha-cpha
  namespace: kube-system

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: typha-cpha
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: typha-cpha
subjects:
  - kind: ServiceAccount
    name: typha-cpha
    namespace: kube-system

---

apiVersion: v1
kind: Service
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    k8s-app: calico-typha
spec:
  ports:
    - port: 5473
      protocol: TCP
      targetPort: calico-typha
      name: calico-typha
  selector:
    k8s-app: calico-typha

---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: bgppeers.crd.projectcalico.org
spec:
  scope: Cluster
  group: crd.projectcalico.org
  version: v1
  names:
    kind: BGPPeer
    plural: bgppeers
    singular: bgppeer
`

const eksHeapsterTpl = `---
apiVersion: v1
kind: List
items:

  - apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      namespace: kube-system
      name: heapster-policy
    spec:
      podSelector:
        matchLabels:
          app: heapster
      ingress:
      - from:
        - namespaceSelector:
            matchLabels:
              heapster: ingress
        - podSelector:
            matchLabels:
              app: heapster
        - podSelector:
            matchLabels:
              heapster-metrics: enabled
        - podSelector:
            matchLabels:
              app: kubernetes-dashboard

  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: heapster-sa
      namespace: kube-system

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: heapster-crb
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: heapster-cr
    subjects:
    - kind: ServiceAccount
      name: heapster-sa
      namespace: kube-system

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: heapster-cr
    rules:
    - apiGroups:
      - ""
      resources:
      - namespaces
      - nodes
      - pods
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/stats
      verbs:
      - create
      - get

    # Heapster's pod_nanny monitors the heapster deployment & its pod(s), and scales
    # the resources of the deployment if necessary.
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: system:pod-nanny-role
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    rules:
    - apiGroups:
      - ""
      resources:
      - pods
      verbs:
      - get
    - apiGroups:
      - "extensions"
      resources:
      - deployments
      verbs:
      - get
      - update

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: heapster-rb
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: system:pod-nanny-role
    subjects:
    - kind: ServiceAccount
      name: heapster-sa
      namespace: kube-system

  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: heapster
      namespace: kube-system
      labels:
        app: heapster
        role: frontend
        kubernetes.io/cluster-service: "true"
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: heapster
      template:
        metadata:
          labels:
            app: heapster
            role: frontend
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ''
        spec:
          serviceAccountName: heapster-sa
          priorityClassName: kube-system-high
          tolerations:
            # Mark the pod as a critical add-on for rescheduling.
            - key: "CriticalAddonsOnly"
              operator: "Exists"
          containers:
            - image: {{ .heapsterImageSrc }}
              name: heapster
              resources:
                # keep request = limit to keep this container in guaranteed class
                # heapster limits provided by: https://github.com/kubernetes/kubernetes/issues/10256
                requests:
                  cpu: 150m
                  memory: 600Mi
                limits:
                  cpu: "150m"
                  memory: 600Mi
              command:
                - /heapster
                - --source=kubernetes.summary_api:https://kubernetes.default?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250&insecure=true
                - --heapster-port=8082
                - --metric-resolution=60s
              ports:
              - containerPort: 8082
                protocol: TCP
                name: summary
            # needed to acess disk io metrics since it uses the legacy source
            # heapster only supports one source at a time, so we need to create another container
            - image: {{ .heapsterImageSrc }}
              name: heapster-legacy
              resources:
                # keep request = limit to keep this container in guaranteed class
                # heapster limits provided by: https://github.com/kubernetes/kubernetes/issues/10256
                requests:
                  cpu: 200m
                  memory: 900Mi
                limits:
                  cpu: 200m
                  memory: 900Mi
              command:
                - /heapster
                - --source=kubernetes:https://kubernetes.default?useServiceAccount=true&kubeletHttps=true&kubeletPort=10250&insecure=true
                - --heapster-port=8083
                - --metric-resolution=60s
              ports:
              - containerPort: 8083
                protocol: TCP
                name: legacy
            - image: {{ .addonResizerImageSrc }}
              name: heapster-nanny
              resources:
                requests:
                  cpu: 50m
                  memory: {{ .heapsterNannyMemory }}
                limits:
                  cpu: 50m
                  memory: {{ .heapsterNannyMemory }}
              env:
                - name: MY_POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: MY_POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              command:
                - /pod_nanny
                - --cpu=150m
                - --extra-cpu=0.5m
                - --memory=600Mi
                - --extra-memory=4Mi
                - --threshold=5
                - --deployment=heapster
                - --container=heapster
                - --poll-period=300000
            # for legacy source
            - image: {{ .addonResizerImageSrc }}
              name: heapster-nanny-legacy
              resources:
                requests:
                  cpu: 100m
                  memory: {{ .heapsterNannyMemory }}
                limits:
                  cpu: 100m
                  memory: {{ .heapsterNannyMemory }}
              env:
                - name: MY_POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: MY_POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              command:
                - /pod_nanny
                - --cpu=200m
                - --extra-cpu=1m
                - --memory=900Mi
                - --extra-memory=8Mi
                - --threshold=5
                - --deployment=heapster
                - --container=heapster-legacy
                - --poll-period=300000

  - kind: Service
    apiVersion: v1
    metadata:
      name: heapster
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: "Heapster"
    spec:
      ports:
        - port: 80
          targetPort: 8082
          name: summary
        - port: 8083
          targetPort: 8083
          name: legacy
      selector:
        app: heapster
`

const eksNetworkPoliciesTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: backend-policy
spec:
  podSelector:
    matchLabels:
      role: backend
  ingress:
      - from:
        - podSelector:
            matchLabels:
              role: frontend

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: dns-policy
  namespace: kube-system
spec:
  ingress:
  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  podSelector:
    matchLabels:
      k8s-app: kube-dns
  policyTypes:
  - Ingress

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      role: frontend 
  ingress:
    - ports:
      # Allow http traffic to frontend pods
      - port: 80
      - port: 443
      - port: 30080
      - port: 30443
      - port: 5000
      - port: 3000
      - port: 5601
      - port: 8080
      - port: 8081
      - port: 8082
      - port: 8083
      - port: 9089
      - port: 9090
      - port: 9093
      - port: 8443
      - port: 5193
      from: []

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: kube-state-metrics-policy
spec:
  podSelector:
    matchLabels:
      app: kube-state-metrics
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kube-state-metrics: ingress
    - podSelector:
        matchLabels:
          app: kube-state-metrics
    - podSelector:
        matchLabels:
          kube-state-metrics: enabled

---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny
  namespace: kube-system
spec:
  podSelector: {}
`

const eventratelimitTpl = `---
kind: Configuration
apiVersion: eventratelimit.admission.k8s.io/v1alpha1
limits:
- type: Namespace
  qps: 50
  burst: 100
  cacheSize: 2000
- type: User
  qps: 10
  burst: 50
`

const frontendPolicyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: frontend-policy
spec:
  podSelector:
    matchLabels:
      role: frontend 
  ingress:
    - ports:
      # Allow http traffic to frontend pods
      - port: 80
      - port: 443
      - port: 30080
      - port: 30443
      - port: 5000
      - port: 3000
      - port: 5601
      - port: 8080
      - port: 8081
      - port: 8082
      - port: 8083
      - port: 9089
      - port: 9090
      - port: 9093
      - port: 8443
      - port: 5193
      from: []
`

const heapsterPolicyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: heapster-policy
spec:
  podSelector:
    matchLabels:
      app: heapster
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          heapster: ingress
    - podSelector:
        matchLabels:
          app: heapster
    - podSelector:
        matchLabels:
          heapster-metrics: enabled
    - podSelector:
        matchLabels:
          app: kubernetes-dashboard
#  - ports:
#    # used for "kubectl top pods --heapster-port=80"
#    - port: 80
#    - port: 8082
#    - port: 8083
`

const kubeStateMetricsPolicyTpl = `---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: kube-system
  name: kube-state-metrics-policy
spec:
  podSelector:
    matchLabels:
      app: kube-state-metrics
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kube-state-metrics: ingress
    - podSelector:
        matchLabels:
          app: kube-state-metrics
    - podSelector:
        matchLabels:
          kube-state-metrics: enabled
`

const kubeStateMetricsTpl = `---
apiVersion: v1
kind: List
items:

  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: kube-state-metrics
      namespace: kube-system

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: kube-state-metrics
    rules:
      - apiGroups: [""]
        resources:
          - configmaps
          - secrets
          - nodes
          - pods
          - services
          - resourcequotas
          - replicationcontrollers
          - limitranges
          - persistentvolumeclaims
          - persistentvolumes
          - namespaces
          - endpoints
        verbs: ["list", "watch"]
      - apiGroups: ["extensions"]
        resources:
          - daemonsets
          - deployments
          - replicasets
        verbs: ["list", "watch"]
      - apiGroups: ["apps"]
        resources:
          - statefulsets
        verbs: ["list", "watch"]
      - apiGroups: ["apps"]
        resources:
          - statefulsets
        verbs: ["list", "watch"]
      - apiGroups: ["batch"]
        resources:
          - cronjobs
          - jobs
        verbs: ["list", "watch"]
      - apiGroups: ["autoscaling"]
        resources:
          - horizontalpodautoscalers
        verbs: ["list", "watch"]
      - apiGroups: ["policy"]
        resources:
          - poddisruptionbudgets
        verbs: ["list", "watch"]

  - apiVersion: rbac.authorization.k8s.io/v1
    # kubernetes versions before 1.8.0 should use rbac.authorization.k8s.io/v1beta1
    kind: Role
    metadata:
      namespace: kube-system
      name: kube-state-metrics-resizer
    rules:
      - apiGroups: [""]
        resources:
          - pods
        verbs: ["get"]
      - apiGroups: ["extensions"]
        resources:
          - deployments
        resourceNames: ["kube-state-metrics"]
        verbs: ["get", "update"]

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: kube-state-metrics
      namespace: kube-system
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: Role
      name: kube-state-metrics-resizer
    subjects:
      - kind: ServiceAccount
        name: kube-state-metrics
        namespace: kube-system

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: kube-state-metrics
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: kube-state-metrics
    subjects:
      - kind: ServiceAccount
        name: kube-state-metrics
        namespace: kube-system

  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: kube-state-metrics
      namespace: kube-system
      labels:
        app: kube-state-metrics
        version: v1.4.0
        role: frontend
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: kube-state-metrics
      template:
        metadata:
          labels:
            app: kube-state-metrics
            version: v1.4.0
            kubernetes.io/cluster-service: "true"
            role: frontend
        spec:
          serviceAccountName: kube-state-metrics
          containers:
            - name: kube-state-metrics
              image: gcr.io/google_containers/kube-state-metrics:v1.4.0
              command:
                - /kube-state-metrics
                - --port=8080
                - --telemetry-port=8081
                - --logtostderr
              ports:
                - name: http-metrics
                  containerPort: 8080
                - name: telemetry
                  containerPort: 8081
              readinessProbe:
                httpGet:
                  path: /healthz
                  port: 8080
                initialDelaySeconds: 5
                timeoutSeconds: 5
              resources:
                # keep request = limit to keep this container in guaranteed class
                requests:
                  cpu: 100m
                  memory: 300Mi
                limits:
                  cpu: 100m
                  memory: 300Mi
            - name: addon-resizer
              image: gcr.io/google_containers/addon-resizer:1.8.3
              resources:
                requests:
                  cpu: 150m
                  memory: 50Mi
                limits:
                  cpu: 150m
                  memory: 50Mi
              env:
                - name: MY_POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: MY_POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              command:
                - /pod_nanny
                - --container=kube-state-metrics
                - --cpu=100m
                - --extra-cpu=1m
                - --memory=100Mi
                - --extra-memory=2Mi
                - --threshold=5
                - --deployment=kube-state-metrics

  - apiVersion: v1
    kind: Service
    metadata:
      name: kube-state-metrics
      namespace: kube-system
      annotations:
        prometheus.io/scrape: "true"
      labels:
        kubernetes.io/cluster-service: "true"
        app: kube-state-metrics
    spec:
      ports:
        - name: http-metrics
          port: 8080
          targetPort: http-metrics
          protocol: TCP
          targetPort: http-metrics
        - name: telemetry
          port: 8081
          protocol: TCP
          targetPort: telemetry
      selector:
        app: kube-state-metrics
`

const kubeSystemCriticalTpl = `---
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: kube-system-critical
value: 1000000000
globalDefault: false
description: |
  Critical pods that are not considered system level and reside in the kube-system namespace.
  This will still get trumped by the system level critical classes.
`

const kubeSystemHighTpl = `---
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: kube-system-high
value: 800000000
globalDefault: false
description: |
  High priority pods that are not considered system level and reside in the kube-system namespace.
  This will still get trumped by the system level and kube-system critical classes.
`

const kubeSystemTpl = `---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cluster-services
  namespace: kube-system
spec:
  scopeSelector:
    matchExpressions:
    - operator : In
      scopeName: PriorityClass
      values:
      - "system-node-critical"
      - "system-cluster-critical"
      - "kube-system-critical"
      - "kube-system-high"
`

const openPolicyAgentTpl = `---

apiVersion: v1
kind: Namespace
metadata:
  name: opa

---

apiVersion: v1
kind: Secret
metadata:
  name: opa-server
  namespace: opa
type: kubernetes.io/tls
data:
  tls.crt: {{ publicKey .certsPath .platform "opa" | base64Encode }}
  tls.key: {{ privateKey .certsPath .platform "opa" | base64Encode }}

---

# Grant OPA/kube-mgmt read-only access to resources. This lets kube-mgmt
# replicate resources into OPA so they can be used in policies.
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: opa-viewer
roleRef:
  kind: ClusterRole
  name: view
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  name: system:serviceaccounts:opa
  apiGroup: rbac.authorization.k8s.io

---

# Define role for OPA/kube-mgmt to update configmaps with policy status.
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: opa
  name: configmap-modifier
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["update", "patch"]

---

# Grant OPA/kube-mgmt role defined above.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: opa
  name: opa-configmap-modifier
roleRef:
  kind: Role
  name: configmap-modifier
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: Group
  name: system:serviceaccounts:opa
  apiGroup: rbac.authorization.k8s.io

---

kind: Service
apiVersion: v1
metadata:
  name: opa
  namespace: opa
spec:
  selector:
    app: opa
  ports:
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: opa
  namespace: opa
  name: opa
spec:
  replicas: 1
  selector:
    matchLabels:
      app: opa
  template:
    metadata:
      labels:
        app: opa
      name: opa
    spec:
      containers:
        # WARNING: OPA is NOT running with an authorization policy configured. This
        # means that clients can read and write policies in OPA. If you are
        # deploying OPA in an insecure environment, be sure to configure
        # authentication and authorization on the daemon. See the Security page for
        # details: https://www.openpolicyagent.org/docs/security.html.
        - name: opa
          image: openpolicyagent/opa:0.11.0
          args:
            - "run"
            - "--server"
            - "--tls-cert-file=/certs/tls.crt"
            - "--tls-private-key-file=/certs/tls.key"
            - "--addr=0.0.0.0:443"
            - "--addr=http://127.0.0.1:8181"
          volumeMounts:
            - readOnly: true
              mountPath: /certs
              name: opa-server
        - name: kube-mgmt
          image: openpolicyagent/kube-mgmt:0.8
          args:
            - "--replicate-cluster=v1/namespaces"
            - "--replicate=extensions/v1beta1/ingresses"
      volumes:
        - name: opa-server
          secret:
            secretName: opa-server

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: opa-default-system-main
  namespace: opa
data:
  main: |
    package system

    import data.kubernetes.admission

    main = {
      "apiVersion": "admission.k8s.io/v1beta1",
      "kind": "AdmissionReview",
      "response": response,
    }

    default response = {"allowed": true}

    response = {
        "allowed": false,
        "status": {
            "reason": reason,
        },
    } {
        reason = concat(", ", admission.deny)
        reason != ""
    }
`

const podSecurityPoliciesTpl = `---
apiVersion: v1
kind: List
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: privileged-psp-user
    rules:
    - apiGroups:
      - policy
      resources:
      - podsecuritypolicies
      resourceNames:
      - privileged
      verbs:
      - use

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
        name: privileged-psp-users
    subjects:
    - kind: Group
      apiGroup: rbac.authorization.k8s.io
      name: privileged-psp-users
    - kind: Group
      name: admin
    - kind: ServiceAccount
      name: default
      namespace: kube-system
    - kind: User
      name: kube-apiserver
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: privileged-psp-user

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: restricted-psp-user
    rules:
    - apiGroups:
      - policy
      resources:
      - podsecuritypolicies
      resourceNames:
      - restricted
      verbs:
      - use

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: restricted-psp-users
    subjects:
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: restricted-psp-users
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: privileged-psp-users
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: restricted-psp-user

  - apiVersion: extensions/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: privileged
      annotations:
        seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    spec:
      privileged: true
      allowPrivilegeEscalation: true
      allowedCapabilities:
      - '*'
      volumes:
      - '*'
      hostNetwork: true
      hostPorts:
      - min: 0
        max: 65535
      hostIPC: true
      hostPID: true
      runAsUser:
        rule: 'RunAsAny'
      seLinux:
        rule: 'RunAsAny'
      supplementalGroups:
        rule: 'RunAsAny'
      fsGroup:
        rule: 'RunAsAny'

  - apiVersion: extensions/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: restricted
      annotations:
        seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
        apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
        seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
        apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
    spec:
      privileged: false
      # Required to prevent escalations to root.
      allowPrivilegeEscalation: false
      # This is redundant with non-root + disallow privilege escalation,
      # but we can provide it for defense in depth.
      requiredDropCapabilities:
        - ALL
      # Allow core volume types.
      volumes:
        - 'configMap'
        - 'emptyDir'
        - 'projected'
        - 'secret'
        - 'downwardAPI'
        # Assume that persistentVolumes set up by the cluster admin are safe to use.
        - 'persistentVolumeClaim'
      hostNetwork: false
      hostIPC: false
      hostPID: false
      runAsUser:
        # Require the container to run without root privileges.
        rule: 'MustRunAsNonRoot'
      seLinux:
        # This policy assumes the nodes are using AppArmor rather than SELinux.
        rule: 'RunAsAny'
      supplementalGroups:
        rule: 'MustRunAs'
        ranges:
          # Forbid adding the root group.
          - min: 1
            max: 65535
      fsGroup:
        rule: 'MustRunAs'
        ranges:
          # Forbid adding the root group.
          - min: 1
            max: 65535
      readOnlyRootFilesystem: false
`

const priorityClassesTpl = `---
apiVersion: v1
kind: List
items:

  - apiVersion: scheduling.k8s.io/v1beta1
    kind: PriorityClass
    metadata:
        name: default
    value: 200000000
    globalDefault: true
    description: |
        Pods not assigned a PriorityClass will be dropped into the default class with value 200000000.
        Pods with lower priority are still possible

  - apiVersion: scheduling.k8s.io/v1beta1
    kind: PriorityClass
    metadata:
      name: kube-system-critical
    value: 1000000000
    globalDefault: false
    description: |
      Critical pods that are not considered system level and reside in the kube-system namespace.
      This will still get trumped by the system level critical classes.

  - apiVersion: scheduling.k8s.io/v1beta1
    kind: PriorityClass
    metadata:
      name: kube-system-high
    value: 800000000
    globalDefault: false
    description: |
      High priority pods that are not considered system level and reside in the kube-system namespace.
      This will still get trumped by the system level and kube-system critical classes.

  - apiVersion: scheduling.k8s.io/v1beta1
    kind: PriorityClass
    metadata:
      name: ceph-critical
    value: 900000000
    globalDefault: false
    description: |
      Critical pods that are not considered system level and reside in the rook-ceph-system namespace.
      This will still get trumped by the system level critical classes.
`

const privilegedPspTpl = `---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
spec:
  privileged: true
  allowPrivilegeEscalation: true
  allowedCapabilities:
  - '*'
  volumes:
  - '*'
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
`

const privilegedRbacTpl = `---
apiVersion: v1
kind: List
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: privileged-psp-user
    rules:
    - apiGroups:
      - policy
      resources:
      - podsecuritypolicies
      resourceNames:
      - privileged
      verbs:
      - use

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
        name: privileged-psp-users
    subjects:
    - kind: Group
      apiGroup: rbac.authorization.k8s.io
      name: privileged-psp-users
    - kind: Group
      name: admin
    - kind: ServiceAccount
      name: default
      namespace: kube-system
    - kind: User
      name: kube-apiserver
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: privileged-psp-user
`

const prometheusServiceTpl = `---
apiVersion: v1
kind: Service
metadata:
  name: rook-prometheus
  namespace: rook-ceph
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30900
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: rook-prometheus
`

const prometheusTpl = `---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: rook-ceph
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
  namespace: rook-ceph
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: rook-ceph
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: rook-prometheus
  namespace: rook-ceph
  labels:
    prometheus: rook-prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: rook
  resources:
    requests:
      memory: 400Mi
`

const resourceQuotasTpl = `---
apiVersion: v1
kind: List
items:

  - apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: cluster-services
      namespace: kube-system
    spec:
      scopeSelector:
        matchExpressions:
        - operator : In
          scopeName: PriorityClass
          values:
          - "system-node-critical"
          - "system-cluster-critical"
          - "kube-system-critical"
          - "kube-system-high"

  - apiVersion: v1
    kind: Namespace
    metadata:
      name: rook-ceph-system

  - apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: cluster-services
      namespace: rook-ceph-system
    spec:
      scopeSelector:
        matchExpressions:
        - operator : In
          scopeName: PriorityClass
          values:
          - "system-node-critical"
          - "system-cluster-critical"
          - "ceph-critical"

  - apiVersion: v1
    kind: Namespace
    metadata:
      name: rook-ceph

  - apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: cluster-services
      namespace: rook-ceph
    spec:
      scopeSelector:
        matchExpressions:
        - operator : In
          scopeName: PriorityClass
          values:
          - "system-node-critical"
          - "system-cluster-critical"
          - "ceph-critical"
`

const restrictedPspTpl = `---
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  # Required to prevent escalations to root.
  allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  requiredDropCapabilities:
    - ALL
  # Allow core volume types.
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    # Assume that persistentVolumes set up by the cluster admin are safe to use.
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Require the container to run without root privileges.
    rule: 'MustRunAsNonRoot'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
`

const restrictedRbacTpl = `---
apiVersion: v1
kind: List
items:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: restricted-psp-user
    rules:
    - apiGroups:
      - policy
      resources:
      - podsecuritypolicies
      resourceNames:
      - restricted
      verbs:
      - use

  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: restricted-psp-users
    subjects:
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: restricted-psp-users
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: privileged-psp-users
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: restricted-psp-user
`

const rookBlockstoreTpl = `apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicated-metadata-pool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: ec-data-pool
  namespace: rook-ceph
spec:
  # Make sure you have enough nodes and OSDs running bluestore to support the replica size or erasure code chunks.
  # For the below settings, you need at least 3 OSDs on different nodes (because the 'failureDomain' is 'host' by default).
  erasureCoded:
    dataChunks: 2
    codingChunks: 1
---
# The nodes that are going to mount the erasure coded RBD block storage must have Linux kernel >= '4.11'.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block-retain
provisioner: ceph.rook.io/block
parameters:
  # If you want to use erasure coded pool with RBD, you need to create two pools (as seen above): one erasure coded and one replicated.
  # You need to specify the replicated pool here in the 'blockPool' parameter, it is used for the metadata of the images.
  # The erasure coded pool must be set as the 'dataBlockPool' parameter below.
  blockPool: replicated-metadata-pool
  dataBlockPool: ec-data-pool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use 'rook' as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use 'ext4'.
  fstype: xfs
reclaimPolicy: Retain
---
# The nodes that are going to mount the erasure coded RBD block storage must have Linux kernel >= '4.11'.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block-delete
provisioner: ceph.rook.io/block
parameters:
  # If you want to use erasure coded pool with RBD, you need to create two pools (as seen above): one erasure coded and one replicated.
  # You need to specify the replicated pool here in the 'blockPool' parameter, it is used for the metadata of the images.
  # The erasure coded pool must be set as the 'dataBlockPool' parameter below.
  blockPool: replicated-metadata-pool
  dataBlockPool: ec-data-pool
  # Specify the namespace of the rook cluster from which to create volumes.
  # If not specified, it will use 'rook' as the default namespace of the cluster.
  # This is also the namespace where the cluster will be
  clusterNamespace: rook-ceph
  # Specify the filesystem type of the volume. If not specified, it will use 'ext4'.
  fstype: xfs
reclaimPolicy: Delete

---
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-mgr-dashboard-external-https
  namespace: rook-ceph
  labels:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
spec:
  ports:
  - name: dashboard
    port: 7665
    protocol: TCP
    targetPort: 7665
  selector:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
  sessionAffinity: None
  type: LoadBalancer`

const rookCephSystemTpl = `---
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph-system
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cluster-services
  namespace: rook-ceph-system
spec:
  scopeSelector:
    matchExpressions:
    - operator : In
      scopeName: PriorityClass
      values:
      - "system-node-critical"
      - "system-cluster-critical"
      - "ceph-critical"

`

const rookCephTpl = `---
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cluster-services
  namespace: rook-ceph
spec:
  scopeSelector:
    matchExpressions:
    - operator : In
      scopeName: PriorityClass
      values:
      - "system-node-critical"
      - "system-cluster-critical"
      - "ceph-critical"
`

const rookClusterTpl = `apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v12 is luminous, v13 is mimic, and v14 is nautilus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v14 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    image: ceph/ceph:v14.2.2-20190722
    # Whether to allow unsupported versions of Ceph. Currently luminous, mimic and nautilus are supported, with the recommendation to upgrade to nautilus.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. Must be specified.
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
  dataDirHostPath: /data/rook/config
  # set the amount of mons to be started
  mon:
    count: 3
    allowMultiplePerNode: false
  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    port: 7665
    # serve the dashboard using SSL
    # ssl: true
  monitoring:
    enabled: true
    rulesNamespace: rook-ceph
  network:
    # toggle to use hostNetwork
    hostNetwork: true
  rbdMirroring:
    # The number of daemons that will perform the rbd mirroring.
    # rbd mirroring must be configured with "rbd mirror" from the rook toolbox.
    workers: 0
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/role
                  operator: In
                  values:
                    - persistent
      # podAffinity:
      # podAntiAffinity:
      tolerations:
        - key: storage
          operator: Equal
          value: "persistent"
          effect: "NoSchedule"
  resources:
  # The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
  #    mgr:
  #      limits:
  #        cpu: "500m"
  #        memory: "1024Mi"
  #      requests:
  #        cpu: "500m"
  #        memory: "1024Mi"
  # The above example requests/limits can also be added to the mon and osd components
  #    mon:
  #    osd:
  #priorityClassNames:#
  #  all: "ceph-critical"#
  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^nvme"
    location:
    config:
      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
      # Set the storeType explicitly only if it is required not to use the default.
      storeType: bluestore
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
      osdsPerDevice: "1" # this value can be overridden at the node or device level
      encryptedDevice: "false" # the default value for this option is "false"
# Cluster level list of directories to use for filestore-based OSD storage. If uncommented, this example would create an OSD under the dataDirHostPath.
#    directories:
#    - path: /var/lib/rook
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#    nodes:
#    - name: "172.17.4.101"
#      directories: # specific directories to use for storage can be specified for each node
#      - path: "/rook/storage-dir"
#      resources:
#        limits:
#          cpu: "500m"
#          memory: "1024Mi"
#        requests:
#          cpu: "500m"
#          memory: "1024Mi"
#    - name: "172.17.4.201"
#      devices: # specific devices to use for storage can be specified for each node
#      - name: "sdb"
#      - name: "nvme01" # multiple osds can be created on high performance devices
#        config:
#          osdsPerDevice: "5"
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."`

const rookCommonTpl = `# Namespace where the operator and other rook resources are created
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph-system  # moved from rook-ceph namespace
---
# OLM: BEGIN CEPH CRD
# The CRD declarations
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephclusters.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephCluster
    listKind: CephClusterList
    plural: cephclusters
    singular: cephcluster
  scope: Namespaced
  version: v1
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            cephVersion:
              properties:
                allowUnsupported:
                  type: boolean
                image:
                  type: string
                name:
                  pattern: ^(luminous|mimic|nautilus)$
                  type: string
            dashboard:
              properties:
                enabled:
                  type: boolean
                urlPrefix:
                  type: string
                port:
                  type: integer
            dataDirHostPath:
              pattern: ^/(\S+)
              type: string
            mon:
              properties:
                allowMultiplePerNode:
                  type: boolean
                count:
                  maximum: 9
                  minimum: 1
                  type: integer
                preferredCount:
                  maximum: 9
                  minimum: 0
                  type: integer
              required:
              - count
            network:
              properties:
                hostNetwork:
                  type: boolean
            storage:
              properties:
                nodes:
                  items: {}
                  type: array
                useAllDevices: {}
                useAllNodes:
                  type: boolean
          required:
          - mon
  additionalPrinterColumns:
    - name: DataDirHostPath
      type: string
      description: Directory used on the K8s nodes
      JSONPath: .spec.dataDirHostPath
    - name: MonCount
      type: string
      description: Number of MONs
      JSONPath: .spec.mon.count
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
    - name: State
      type: string
      description: Current State
      JSONPath: .status.state
    - name: Health
      type: string
      description: Ceph Health
      JSONPath: .status.ceph.health
# OLM: END CEPH CRD
---
# OLM: BEGIN CEPH FS CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephfilesystems.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephFilesystem
    listKind: CephFilesystemList
    plural: cephfilesystems
    singular: cephfilesystem
  scope: Namespaced
  version: v1
  additionalPrinterColumns:
    - name: MdsCount
      type: string
      description: Number of MDSs
      JSONPath: .spec.metadataServer.activeCount
    - name: Age
      type: date
      JSONPath: .metadata.creationTimestamp
# OLM: END CEPH FS CRD
---
# OLM: BEGIN CEPH NFS CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephnfses.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephNFS
    listKind: CephNFSList
    plural: cephnfses
    singular: cephnfs
    shortNames:
    - nfs
  scope: Namespaced
  version: v1
# OLM: END CEPH NFS CRD
---
# OLM: BEGIN CEPH OBJECT STORE CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephobjectstores.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephObjectStore
    listKind: CephObjectStoreList
    plural: cephobjectstores
    singular: cephobjectstore
  scope: Namespaced
  version: v1
# OLM: END CEPH OBJECT STORE CRD
---
# OLM: BEGIN CEPH OBJECT STORE USERS CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephobjectstoreusers.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephObjectStoreUser
    listKind: CephObjectStoreUserList
    plural: cephobjectstoreusers
    singular: cephobjectstoreuser
  scope: Namespaced
  version: v1
# OLM: END CEPH OBJECT STORE USERS CRD
---
# OLM: BEGIN CEPH BLOCK POOL CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: cephblockpools.ceph.rook.io
spec:
  group: ceph.rook.io
  names:
    kind: CephBlockPool
    listKind: CephBlockPoolList
    plural: cephblockpools
    singular: cephblockpool
  scope: Namespaced
  version: v1
# OLM: END CEPH BLOCK POOL CRD
---
# OLM: BEGIN CEPH VOLUME POOL CRD
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: volumes.rook.io
spec:
  group: rook.io
  names:
    kind: Volume
    listKind: VolumeList
    plural: volumes
    singular: volume
    shortNames:
    - rv
  scope: Namespaced
  version: v1alpha2
# OLM: END CEPH VOLUME POOL CRD
---
# OLM: BEGIN OPERATOR ROLE
# The cluster role for managing all the cluster-specific resources in a namespace
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: rook-ceph-cluster-mgmt
  labels:
    operator: rook
    storage-backend: ceph
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.ceph.rook.io/aggregate-to-rook-ceph-cluster-mgmt: "true"
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: rook-ceph-cluster-mgmt-rules
  labels:
    operator: rook
    storage-backend: ceph
    rbac.ceph.rook.io/aggregate-to-rook-ceph-cluster-mgmt: "true"
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  - pods
  - pods/log
  - services
  - configmaps
  verbs:
  - get
  - list
  - watch
  - patch
  - create
  - update
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
---
# The role for the operator to manage resources in its own namespace
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
  labels:
    operator: rook
    storage-backend: ceph
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  - services
  verbs:
  - get
  - list
  - watch
  - patch
  - create
  - update
  - delete
- apiGroups:
  - apps
  resources:
  - daemonsets
  - statefulsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
---
# The cluster role for managing the Rook CRDs
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: rook-ceph-global
  labels:
    operator: rook
    storage-backend: ceph
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.ceph.rook.io/aggregate-to-rook-ceph-global: "true"
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: rook-ceph-global-rules
  labels:
    operator: rook
    storage-backend: ceph
    rbac.ceph.rook.io/aggregate-to-rook-ceph-global: "true"
rules:
- apiGroups:
  - ""
  resources:
  # Pod access is needed for fencing
  - pods
  # Node access is needed for determining nodes where mons should run
  - nodes
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
    # PVs and PVCs are managed by the Rook provisioner
  - persistentvolumes
  - persistentvolumeclaims
  - endpoints
  verbs:
  - get
  - list
  - watch
  - patch
  - create
  - update
  - delete
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - ceph.rook.io
  resources:
  - "*"
  verbs:
  - "*"
- apiGroups:
  - rook.io
  resources:
  - "*"
  verbs:
  - "*"
---
# Aspects of ceph-mgr that require cluster-wide access
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-cluster
  labels:
    operator: rook
    storage-backend: ceph
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.ceph.rook.io/aggregate-to-rook-ceph-mgr-cluster: "true"
rules: []
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-cluster-rules
  labels:
    operator: rook
    storage-backend: ceph
    rbac.ceph.rook.io/aggregate-to-rook-ceph-mgr-cluster: "true"
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - nodes
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
---
# OLM: END OPERATOR ROLE
# OLM: BEGIN SERVICE ACCOUNT SYSTEM
# The rook system service account used by the operator, agent, and discovery pods
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
  labels:
    operator: rook
    storage-backend: ceph
---
# OLM: END SERVICE ACCOUNT SYSTEM
# OLM: BEGIN OPERATOR ROLEBINDING
# Grant the operator, agent, and discovery agents access to resources in the namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
  labels:
    operator: rook
    storage-backend: ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-system
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
---
# Grant the rook system daemons cluster-wide access to manage the Rook CRDs, PVCs, and storage classes
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-global
  namespace: rook-ceph-system  # moved from rook-ceph namespace
  labels:
    operator: rook
    storage-backend: ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-global
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
# OLM: END OPERATOR ROLEBINDING
---
#################################################################################################################
# Beginning of cluster-specific resources. The example will assume the cluster will be created in the "rook-ceph"
# namespace. If you want to create the cluster in a different namespace, you will need to modify these roles
# and bindings accordingly.
#################################################################################################################
# Namespace where the cluster specific resources are created
apiVersion: v1
kind: Namespace
metadata:
  name: rook-ceph
---
# OLM: BEGIN SERVICE ACCOUNT OSD
# Service account for the Ceph OSDs. Must exist and cannot be renamed.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-osd
  namespace: rook-ceph
# OLM: END SERVICE ACCOUNT OSD
---
# OLM: BEGIN SERVICE ACCOUNT MGR
# Service account for the Ceph Mgr. Must exist and cannot be renamed.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-mgr
  namespace: rook-ceph
# OLM: END SERVICE ACCOUNT MGR
---
# OLM: BEGIN CMD REPORTER SERVICE ACCOUNT
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-cmd-reporter
  namespace: rook-ceph
# OLM: END CMD REPORTER SERVICE ACCOUNT
---
# OLM: BEGIN CLUSTER ROLE
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-osd
  namespace: rook-ceph
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: [ "get", "list", "watch", "create", "update", "delete" ]
---
# Aspects of ceph-mgr that require access to the system namespace
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-system
  namespace: rook-ceph
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.ceph.rook.io/aggregate-to-rook-ceph-mgr-system: "true"
rules: []
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-system-rules
  namespace: rook-ceph
  labels:
      rbac.ceph.rook.io/aggregate-to-rook-ceph-mgr-system: "true"
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
---
# Aspects of ceph-mgr that operate within the cluster's namespace
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr
  namespace: rook-ceph
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - ceph.rook.io
  resources:
  - "*"
  verbs:
  - "*"
# OLM: END CLUSTER ROLE
---
# OLM: BEGIN CMD REPORTER ROLE
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cmd-reporter
  namespace: rook-ceph
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
# OLM: END CMD REPORTER ROLE
---
# OLM: BEGIN CLUSTER ROLEBINDING
# Allow the operator to create resources in this cluster's namespace
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: rook-ceph-system  # moved from rook-ceph namespace
---
# Allow the osd pods in this namespace to work with configmaps
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-osd
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-osd
subjects:
- kind: ServiceAccount
  name: rook-ceph-osd
  namespace: rook-ceph
---
# Allow the ceph mgr to access the cluster-specific resources necessary for the mgr modules
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-mgr
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: rook-ceph
---
# Allow the ceph mgr to access the rook system resources necessary for the mgr modules
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-system
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-mgr-system
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: rook-ceph
---
# Allow the ceph mgr to access cluster-wide resources necessary for the mgr modules
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-mgr-cluster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-mgr-cluster
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: rook-ceph
# OLM: END CLUSTER ROLEBINDING
---
# OLM: BEGIN CMD REPORTER ROLEBINDING
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: rook-ceph-cmd-reporter
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cmd-reporter
subjects:
- kind: ServiceAccount
  name: rook-ceph-cmd-reporter
  namespace: rook-ceph
# OLM: END CMD REPORTER ROLEBINDING
---
#################################################################################################################
# Beginning of pod security policy resources. The example will assume the cluster will be created in the
# "rook-ceph" namespace. If you want to create the cluster in a different namespace, you will need to modify
# the roles and bindings accordingly.
#################################################################################################################
# OLM: BEGIN CLUSTER POD SECURITY POLICY
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: rook-privileged
spec:
  privileged: true
  allowedCapabilities:
    # required by CSI
    - SYS_ADMIN
  # fsGroup - the flexVolume agent has fsGroup capabilities and could potentially be any group
  fsGroup:
    rule: RunAsAny
  # runAsUser, supplementalGroups - Rook needs to run some pods as root
  # Ceph pods could be run as the Ceph user, but that user isn't always known ahead of time
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  # seLinux - seLinux context is unknown ahead of time; set if this is well-known
  seLinux:
    rule: RunAsAny
  volumes:
    # recommended minimum set
    - configMap
    - downwardAPI
    - emptyDir
    - persistentVolumeClaim
    - secret
    - projected
    # required for Rook
    - hostPath
    - flexVolume
  # allowedHostPaths can be set to Rook's known host volume mount points when they are fully-known
  # directory-based OSDs make this hard to nail down
  # allowedHostPaths:
  #   - /run/udev      # for OSD prep
  #   - /dev           # for OSD prep
  #   - /var/lib/rook  # or whatever the dataDirHostPath value is set to
  # Ceph requires host IPC for setting up encrypted devices
  hostIPC: true
  # Ceph OSDs need to share the same PID namespace
  hostPID: true
  # hostNetwork can be set to 'false' if host networking isn't used
  hostNetwork: true
  hostPorts:
    # Ceph messenger protocol v1
    - min: 6789
      max: 6790 # <- support old default port
    # Ceph messenger protocol v2
    - min: 3300
      max: 3300
    # Ceph RADOS ports for OSDs, MDSes
    - min: 6800
      max: 7300
    # # Ceph dashboard port HTTP (not recommended)
    # - min: 7000
    #   max: 7000
    # Ceph dashboard port HTTPS
    - min: 8443
      max: 8443
    # Ceph mgr Prometheus Metrics
    - min: 9283
      max: 9283
# OLM: END CLUSTER POD SECURITY POLICY
---
# OLM: BEGIN POD SECURITY POLICY BINDINGS
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: 'psp:rook'
rules:
  - apiGroups:
      - policy
    resources:
      - podsecuritypolicies
    resourceNames:
      - rook-privileged
    verbs:
      - use
---
# Allow the rook-ceph-system serviceAccount to use the privileged PSP
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rook-ceph-system-psp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: 'psp:rook'
subjects:
  - kind: ServiceAccount
    name: rook-ceph-system
    namespace: rook-ceph-system  # changed from rook-ceph
---
# Allow the default serviceAccount to use the privileged PSP
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-default-psp
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:rook
subjects:
- kind: ServiceAccount
  name: default
  namespace: rook-ceph
---
# Allow the rook-ceph-osd serviceAccount to use the privileged PSP
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-osd-psp
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:rook
subjects:
- kind: ServiceAccount
  name: rook-ceph-osd
  namespace: rook-ceph
---
# Allow the rook-ceph-mgr serviceAccount to use the privileged PSP
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-mgr-psp
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:rook
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: rook-ceph
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-cmd-reporter-psp
  namespace: rook-ceph
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp:rook
subjects:
- kind: ServiceAccount
  name: rook-ceph-cmd-reporter
  namespace: rook-ceph
# OLM: END CLUSTER POD SECURITY POLICY BINDINGS
---`

const rookFilestoreTpl = `apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: rook-global-filestore
  namespace: rook-ceph
spec:
  # The metadata pool spec
  metadataPool:
    replicated:
      # You need at least three OSDs on different nodes for this config to work
      size: 3
  # The list of data pool specs
  dataPools:
    # You need at least three 'bluestore' OSDs on different nodes for this config to work
    - erasureCoded:
        dataChunks: 2
        codingChunks: 1
  # The metadata service (mds) configuration
  metadataServer:
    # The number of active MDS instances
    activeCount: 1
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
    activeStandby: true
    # The affinity rules to apply to the mds deployment
    placement:
    #  nodeAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role
    #          operator: In
    #          values:
    #          - mds-node
    #  tolerations:
    #  - key: mds-node
    #    operator: Exists
    #  podAffinity:
    #  podAntiAffinity:
    resources:
    # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #priorityClassName: "ceph-critical"#`

const rookObjectUserTpl = `---
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: default-user
  namespace: rook-ceph
spec:
  store: rook-s3-object-store
  displayName: "default user"
`

const rookObjectstoreTpl = `---
apiVersion: v1
kind: Secret
metadata:
  name: rook-s3-object-store-pem
  namespace: rook-ceph
type: Opaque
data:
  cert: {{ cert .certsPath .platform "ingress" }}
---
apiVersion: v1
kind: Secret
metadata:
  name: rook-s3-object-store-ca
  namespace: rook-ceph
type: Opaque
data:
  cert: {{ publicKey .certsPath .platform "ingress_root_ca" | base64Encode }}
---
apiVersion: ceph.rook.io/v1
kind: CephObjectStore
metadata:
  name: rook-s3-object-store
  namespace: rook-ceph
spec:
  # The pool spec used to create the metadata pools
  metadataPool:
    failureDomain: host
    replicated:
      size: 3
  # The pool spec used to create the data pool
  dataPool:
    failureDomain: host
    # If you have at least three osds, erasure coding can be specified
    erasureCoded:
      dataChunks: 2
      codingChunks: 1
  # The gaeteway service configuration
  gateway:
    # type of the gateway (s3)
    type: s3
    # A reference to the secret in the rook namespace where the ssl certificate is stored
    sslCertificateRef: rook-s3-object-store-pem
    # The port that RGW pods will listen on (http)
    port: 80
    # The port that RGW pods will listen on (https). An ssl certificate is required.
    securePort: 443
    # The number of pods in the rgw deployment (ignored if allNodes=true)
    instances: 2
    # Whether the rgw pods should be deployed on all nodes as a daemonset
    allNodes: false
    # The affinity rules to apply to the rgw deployment or daemonset.
    placement:
    #  nodeAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      nodeSelectorTerms:
    #      - matchExpressions:
    #        - key: role
    #          operator: In
    #          values:
    #          - rgw-node
    #  tolerations:
    #  - key: rgw-node
    #    operator: Exists
    #  podAffinity:
    #  podAntiAffinity:
    resources:
    # The requests and limits set here, allow the object store gateway Pod(s) to use half of one CPU core and 1 gigabyte of memory
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
---
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: rook-s3-object-store-default-user
  namespace: rook-ceph
spec:
  store: rook-s3-object-store
  displayName: "default user"
---
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-rgw-rook-s3-object-store-external
  namespace: rook-ceph
  labels:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph
    rook_object_store: rook-s3-object-store
spec:
  ports:
  - name: rgw
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: rook-ceph-rgw
    rook_cluster: rook-ceph
    rook_object_store: rook-s3-object-store
  sessionAffinity: None
  type: NodePort
`

const rookOperatorTpl = `# OLM: BEGIN OPERATOR DEPLOYMENT
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rook-ceph-operator
  namespace: rook-ceph-system  # moved from rook-ceph namespace
  labels:
    operator: rook
    storage-backend: ceph
spec:
  selector:
    matchLabels:
      app: rook-ceph-operator
  replicas: 1
  template:
    metadata:
      labels:
        app: rook-ceph-operator
    spec:
    #  priorityClassName: ceph-critical#
      serviceAccountName: rook-ceph-system
      containers:
      - name: rook-ceph-operator
        image: rook/ceph:v1.0.6
        args: ["ceph", "operator"]
        volumeMounts:
        - mountPath: /var/lib/rook
          name: rook-config
        - mountPath: /etc/ceph
          name: default-config-dir
        env:
        # If the operator should only watch for cluster CRDs in the same namespace, set this to "true".
        # If this is not set to true, the operator will watch for cluster CRDs in all namespaces.
        - name: ROOK_CURRENT_NAMESPACE_ONLY
          value: "false"
        # To disable RBAC, uncomment the following:
        - name: RBAC_ENABLED
          value: "true"
        # Rook Agent toleration. Will tolerate all taints with all keys.
        # Choose between NoSchedule, PreferNoSchedule and NoExecute:
        - name: AGENT_TOLERATION
          value: "NoSchedule"
        # (Optional) Rook Agent toleration key. Set this to the key of the taint you want to tolerate
        - name: AGENT_TOLERATION_KEY
          value: "storage"
        #- name: AGENT_PRIORITY_CLASS_NAME#
        #  value: "ceph-critical"#
        # (Optional) Rook Agent mount security mode. Can by "Any" or "Restricted".
        # "Any" uses Ceph admin credentials by default/fallback.
        # For using "Restricted" you must have a Ceph secret in each namespace storage should be consumed from and
        # set "mountUser" to the Ceph user, "mountSecret" to the Kubernetes secret name.
        # to the namespace in which the "mountSecret" Kubernetes secret namespace.
        # - name: AGENT_MOUNT_SECURITY_MODE
        #   value: "Any"
        # Set the path where the Rook agent can find the flex volumes
        #- name: FLEXVOLUME_DIR_PATH
        #  value: "/var/lib/kubelet/volumeplugins"
        # Set the path where kernel modules can be found
        # - name: LIB_MODULES_DIR_PATH
        #  value: "<PathToLibModules>"
        # Mount any extra directories into the agent container
        # - name: AGENT_MOUNTS
        #  value: "somemount=/host/path:/container/path,someothermount=/host/path2:/container/path2"
        # Rook Discover toleration. Will tolerate all taints with all keys.
        # Choose between NoSchedule, PreferNoSchedule and NoExecute:
        - name: DISCOVER_TOLERATION
          value: "NoSchedule"
        # (Optional) Rook Discover toleration key. Set this to the key of the taint you want to tolerate
        - name: DISCOVER_TOLERATION_KEY
          value: "storage"
        #- name: DISCOVER_PRIORITY_CLASS_NAME#
        #  value: "ceph-critical"#
        # Allow rook to create multiple file systems. Note: This is considered
        # an experimental feature in Ceph as described at
        # http://docs.ceph.com/docs/master/cephfs/experimental-features/#multiple-filesystems-within-a-ceph-cluster
        # which might cause mons to crash as seen in https://github.com/rook/rook/issues/1027
        - name: ROOK_ALLOW_MULTIPLE_FILESYSTEMS
          value: "false"
        # The logging level for the operator: INFO | DEBUG
        - name: ROOK_LOG_LEVEL
          value: "INFO"
        # The interval to check the health of the ceph cluster and update the status in the custom resource.
        - name: ROOK_CEPH_STATUS_CHECK_INTERVAL
          value: "60s"
        # The interval to check if every mon is in the quorum.
        - name: ROOK_MON_HEALTHCHECK_INTERVAL
          value: "45s"
        # The duration to wait before trying to failover or remove/replace the
        # current mon with a new mon (useful for compensating flapping network).
        - name: ROOK_MON_OUT_TIMEOUT
          value: "300s"
        # The duration between discovering devices in the rook-discover daemonset.
        - name: ROOK_DISCOVER_DEVICES_INTERVAL
          value: "5m"
        # Whether to start pods as privileged that mount a host path, which includes the Ceph mon and osd pods.
        # This is necessary to workaround the anyuid issues when running on OpenShift.
        # For more details see https://github.com/rook/rook/issues/1314#issuecomment-355799641
        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
          value: "false"
        # In some situations SELinux relabelling breaks (times out) on large filesystems, and doesn't work with cephfs ReadWriteMany volumes (last relabel wins).
        # Disable it here if you have similar issues.
        # For more details see https://github.com/rook/rook/issues/2417
        - name: ROOK_ENABLE_SELINUX_RELABELING
          value: "true"
        # In large volumes it will take some time to chown all the files. Disable it here if you have performance issues.
        # For more details see https://github.com/rook/rook/issues/2254
        - name: ROOK_ENABLE_FSGROUP
          value: "true"
        # Disable automatic orchestration when new devices are discovered
        - name: ROOK_DISABLE_DEVICE_HOTPLUG
          value: "false"
        # The name of the node to pass with the downward API
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # The pod name to pass with the downward API
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # The pod namespace to pass with the downward API
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      volumes:
      - name: rook-config
        emptyDir: {}
      - name: default-config-dir
        emptyDir: {}
# OLM: END OPERATOR DEPLOYMENT`

const rookStorageClassTpl = `---
apiVersion: v1
kind: List
items:

  - apiVersion: ceph.rook.io/v1
    kind: CephBlockPool
    metadata:
      name: replicapool
      namespace: rook-ceph
    spec:
      replicated:
        size: 1

  - apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: rook-ceph-block-retain
      annotations:
{% if default_storageclass == "rook-ceph-block-retain" %}
        storageclass.kubernetes.io/is-default-class: "true"
{% endif %}
    provisioner: ceph.rook.io/block
    parameters:
      blockPool: replicapool
      # Specify the namespace of the rook cluster from which to create volumes.
      # If not specified, it will use 'rook' as the default namespace of the cluster.
      # This is also the namespace where the cluster will be
      clusterNamespace: rook-ceph
      # Specify the filesystem type of the volume. If not specified, it will use 'ext4'.
      fstype: xfs
      # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
      #mountUser: user1
      # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
      # The secret must exist in each namespace(s) where the storage will be consumed.
      #mountSecret: ceph-user1-secret
    reclaimPolicy: Retain

  - apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: rook-ceph-block-delete
      annotations:
{% if default_storageclass == "rook-ceph-block-delete" %}
        storageclass.kubernetes.io/is-default-class: "true"
{% endif %}
    provisioner: ceph.rook.io/block
    parameters:
      blockPool: replicapool
      # Specify the namespace of the rook cluster from which to create volumes.
      # If not specified, it will use 'rook' as the default namespace of the cluster.
      # This is also the namespace where the cluster will be
      clusterNamespace: rook-ceph
      # Specify the filesystem type of the volume. If not specified, it will use 'ext4'.
      fstype: xfs
      # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
      #mountUser: user1
      # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
      # The secret must exist in each namespace(s) where the storage will be consumed.
      #mountSecret: ceph-user1-secret
    reclaimPolicy: Delete

`

const rookToolboxTpl = `---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rook-ceph-tools
  namespace: rook-ceph
  labels:
    app: rook-ceph-tools
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rook-ceph-tools
  template:
    metadata:
      labels:
        app: rook-ceph-tools
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: rook-ceph-tools
        image: rook/ceph:v0.9.3
        command: ["/tini"]
        args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
        imagePullPolicy: IfNotPresent
        env:
          - name: ROOK_ADMIN_SECRET
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: admin-secret
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /dev
            name: dev
          - mountPath: /sys/bus
            name: sysbus
          - mountPath: /lib/modules
            name: libmodules
          - name: mon-endpoint-volume
            mountPath: /etc/rook
      # if hostNetwork: false, the "rbd map" command hangs, see https://github.com/rook/rook/issues/2021
      hostNetwork: true
      volumes:
        - name: dev
          hostPath:
            path: /dev
        - name: sysbus
          hostPath:
            path: /sys/bus
        - name: libmodules
          hostPath:
            path: /lib/modules
        - name: mon-endpoint-volume
          configMap:
            name: rook-ceph-mon-endpoints
            items:
            - key: data
              path: mon-endpoints
`

const serviceMonitorTpl = `---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: rook-ceph-mgr
  namespace: rook-ceph
  labels:
    team: rook
spec:
  namespaceSelector:
    matchNames:
      - rook-ceph
  selector:
    matchLabels:
      app: rook-ceph-mgr
      rook_cluster: rook-ceph
  endpoints:
  - port: http-metrics
    path: /metrics
    interval: 5s
`

const vsphereVolumesTpl = `apiVersion: v1
kind: Secret
metadata:
  name: vcconf
  namespace: kube-system
type: Opaque
data:
   {{ .vsphereServer }}.username: {{ base64Encode .vsphereUsername }}
   {{ .vsphereServer }}.password: {{ base64Encode .vspherePassword }}

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata: 
  name: vsphere-block-thick
provisioner: kubernetes.io/vsphere-volume
parameters: 
  # datastore: <relying on global default in vsphere.conf, but could have custom classes for apps/users/etc>
  diskformat: zeroedthick
  fstype: ext4

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata: 
  name: vsphere-block-thin
provisioner: kubernetes.io/vsphere-volume
parameters: 
  # datastore: <relying on global default in vsphere.conf, but could have custom classes for apps/users/etc>
  diskformat: thin
  fstype: ext4
`
