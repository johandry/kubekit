force_apply: yes
force_apply_flag: "{% if force_apply|bool %}--force {% endif %}"

kube_api_server_local: "https://localhost:{{ kube_api_ssl_port }}"
kube_api_ssl_port: "6558"

kube_first_master_fqdn: "{{ hostvars[ 'master000']['fqdn'] | lower }}"
registry_port: 5000
registry_lb_port: 5005
docker_mtu: "{{ hostvars[inventory_hostname][cluster_iface_name]['mtu'] | default(1500) }}"
kube_cluster_dns: "172.21.0.10"

core_repo_root: "{{ kube_virtual_ip_api if is_ha_cluster else (hostvars['master000'][address_inventory_field] if cloud_provider != 'ec2' else hostvars['master000']['private_ip']) }}:{{ registry_lb_port if is_ha_cluster else registry_port }}/tdc/"

cluster_iface_name: "ansible_eth0"
cluster_iface: "{{ hostvars[inventory_hostname][cluster_iface_name] }}"
cni_iface: "{{ cluster_iface.device }}"
cni_ip_encapsulation: "Always" # can be either ("Off" or "Always")
kube_cluster_cidr: "172.24.0.0/16"

nginx_ingress_enabled: no
dns_aaaa_delay_enabled: yes

core_manifests:
  - name: control-plane-services
    enabled: yes
    prerequisite: yes
  - name: calico
    enabled: yes
    prerequisite: yes
  - name: coredns
    enabled: yes
    prerequisite: yes
  - name: dns-aaaa-delay
    enabled: "{{ dns_aaaa_delay_enabled | bool }}"
    prerequisite: yes
  - name: kube-state-metrics
    enabled: yes
    prerequisite: no
  - name: coredns-autoscaler
    enabled: yes
    prerequisite: no


cert_dir: /etc/pki
ingress_root_ca_crt_filename: ingress_root_ca.crt
ingress_cert_key_filename: ingress.key
ingress_cert_crt_filename: ingress.crt
ingress_cert_csr_filename: ingress.csr

root_ca_crt_filename: root_ca.crt
root_ca_key_filename: root_ca.key

etcd_root_ca_crt_filename: etcd_root_ca.crt

certs:
  admin:
    key:  admin.key
    crt:  admin.crt
    pem:  admin.pem
  etcd:
    key:  etcd_node.key
    crt:  etcd_node.crt
    pem:  etcd_node.pem

ingress_additional_dns_alt_names: []
ingress_additional_dns_alt_ips: []
ingress_default_dns_alt_names:
- "localhost"
ingress_default_dns_alt_ips:
- "127.0.0.1"

nginx_ingress_controller_node_affinity_ansible_group: "master"
is_ha_cluster: "{{ groups['master'] | length > 1 and (not disable_master_ha | bool) }}"

kubelet_max_pods: 110

# general rook availabity, nothing rook will be this false
# automatically includes block store when true 
rook_enabled: yes
rook_csi_enabled: no
rook_monitoring_enabled: yes

# enable ceph dashboard
rook_dashboard_enabled: yes
rook_dashboard_external_enabled: no
rook_dashboard_port: 7665

# enable s3 inerface 
rook_object_store_enabled: yes
rook_object_store_rados_gateway_enabled: yes

# enable filestore
rook_file_store_enabled: yes

rook_manifests:
  - name: rook-common
    enabled: "{{ rook_enabled | bool }}"
    operator: yes
    folder: cluster
  - name: rook-operator
    enabled: "{{ rook_enabled | bool }}"
    operator: yes
    folder: cluster
  - name: rook-cluster
    enabled: "{{ rook_enabled | bool }}"
    operator: no
    folder: cluster
  - name: rook-pool
    enabled: "{{ rook_enabled | bool }}"
    operator: no
    folder: storage
  - name: rook-storage-class
    enabled: "{{ rook_enabled | bool }}"
    operator: no
    folder: storage
  - name: rook-object-store
    enabled: "{{ (rook_enabled | bool) and (rook_object_store_enabled | bool) }}"
    operator: no
    # temporary workaround pending rook fix to have object store user queue if object store pending
    # public issue and pr number to follow
    folder: storage
  - name: rook-object-user
    enabled: no # "{{ (rook_enabled | bool) and (rook_object_store_enabled | bool) }}"
    operator: no
    folder: storage
  - name: rook-filesystem
    enabled: "{{ (rook_enabled | bool) and (rook_file_store_enabled | bool) }}"
    operator: no
    folder: storage
  - name: rook-toolbox
    enabled: no
    operator: no
    folder: toolbox


rook_ceph_monitor_count: 3
rook_ceph_monitor_allow_multiple_per_node: no
rook_ceph_use_host_network: yes

# placement options for ceph pods, should be rendered to single line json
rook_ceph_placement_options_all: ""
rook_ceph_placement_options_mon: ""
rook_ceph_placement_options_osd: ""
rook_ceph_placement_options_mgr: ""
# To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
# The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage' and
# tolerate taints with a key of 'storage-node'.
# example rendered content in yaml
#          nodeAffinity:
#            requiredDuringSchedulingIgnoredDuringExecution:
#              nodeSelectorTerms:
#              - matchExpressions:
#                - key: role
#                  operator: In
#                  values:
#                  - storage-node
#          podAffinity:
#          podAntiAffinity:
#          tolerations:
#          - key: storage-node
#            operator: Exists

rook_config_data_dir: /etc/kubernetes/rook

rook_ceph_annotation_options_all: ""
rook_ceph_annotation_options_mon: ""
rook_ceph_annotation_options_osd: ""
rook_ceph_annotation_options_mgr: ""
#rook_ceph_all_priority_class: ceph-critical

rook_ceph_mgr_resource_limits_cpu: 1000m
rook_ceph_mgr_resource_limits_mem: 4096Mi
rook_ceph_mgr_resource_requests_cpu: 1000m
rook_ceph_mgr_resource_requests_mem: 4096Mi
#rook_ceph_mgr_priority_class: ceph-critical

rook_ceph_mon_resource_limits_cpu: 500m
rook_ceph_mon_resource_limits_mem: 1024Mi
rook_ceph_mon_resource_requests_cpu: 500m
rook_ceph_mon_resource_requests_mem: 1024Mi
#rook_ceph_mon_priority_class: ceph-critical

# if rook OSDs have less than 4GB, then you get the error saying:
# "failed to create cluster in namespace rook-ceph. failed to start the osds. refuse to run the pod with 2048mb of ram, provide at least 4096mb."
rook_ceph_osd_resource_limits_cpu: 1000m
rook_ceph_osd_resource_limits_mem: 4096Mi
rook_ceph_osd_resource_requests_cpu: 1000m
rook_ceph_osd_resource_requests_mem: 4096Mi

rook_ceph_storage_use_all_nodes: yes
rook_ceph_storage_use_all_devices: no

# storage devices to give to direclty to ceph,
# for example "^nvme0n1" can be used on i3 type nodes in ec2
# to hand over the nvme first drive to ceph as an osd
rook_ceph_storage_device_filter: ""
rook_ceph_storage_location: ""
#rook_ceph_storage_priority_class: ceph-critical


# enables host directories to be used as osd drives
# performance penalties apply
rook_ceph_storage_directories:
  -  /data/rook/storage/0

rook_ceph_filesystem_meta_pool_replication_count: 3
rook_ceph_filesystem_data_pool_replication_count: 3
rook_ceph_filesystem_mds_count: 1
rook_ceph_filesystem_active_standby: yes
rook_ceph_filesystem_placement_options: ""
rook_ceph_filesystem_annotation_options: ""
#rook_ceph_filesystem_priority_class: ceph-critical

# placement options for ceph filesystem pods, should be rendered to single line json
# example rendered content in yaml

#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#      - matchExpressions:
#        - key: role
#          operator: In
#          values:
#          - mds-node
#  tolerations:
#  - key: mds-node
#    operator: Exists
#  podAffinity:
#  podAntiAffinity:

# filestore requires at least 4096mb memory or else you get the following error:
# "failed to create filesystem rook-global-filestore: refuse to run the pod with 1024mb of ram, provide at least 4096mb."
rook_ceph_filesystem_resource_limits_cpu: 500m
rook_ceph_filesystem_resource_limits_mem: 4096Mi
rook_ceph_filesystem_resource_requests_cpu: 500m
rook_ceph_filesystem_resource_requests_mem: 4096Mi

rook_ceph_object_store_meta_pool_replication_count: 3
rook_ceph_object_store_data_pool_replication_count: 3
rook_ceph_object_store_gateway_type: s3
rook_ceph_object_store_instance_count: 1
rook_ceph_object_store_all_nodes: no
rook_ceph_object_store_resource_limits_cpu: 1000m
rook_ceph_object_store_resource_limits_mem: 2048Mi
rook_ceph_object_store_resource_requests_cpu: 1000m
rook_ceph_object_store_resource_requests_mem: 2048Mi

rook_ceph_object_store_placement: ""
rook_ceph_object_store_annotations: ""
#rook_ceph_object_store_priority_class: ceph-critical

# placement options for ceph objectstore pods, should be rendered to single line json
# example rendered content in yaml

#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#      - matchExpressions:
#        - key: role
#          operator: In
#          values:
#          - rgw-node
#  tolerations:
#  - key: rgw-node
#    operator: Exists
#  podAffinity:
#  podAntiAffinity:

rook_ceph_replica_pool_count: 3
rook_ceph_replica_pool_annotations: ""
rook_ceph_storage_class_replication_count: 3

# resource settings
# -------------------------

# calico
calico_node_requests_cpu: "250m"
calico_node_requests_memory: "256Mi"
calico_node_limits_cpu: "{{ calico_node_requests_cpu }}"
calico_node_limits_memory: "{{ calico_node_requests_memory }}"

install_cni_requests_cpu: "50m"
install_cni_requests_memory: "16Mi"
install_cni_limits_cpu: "{{ install_cni_requests_cpu }}"
install_cni_limits_memory: "{{ install_cni_requests_memory }}"

calico_typha_requests_cpu: "250m"
calico_typha_requests_memory: "256Mi"
calico_typha_limits_cpu: "{{ calico_typha_requests_cpu }}"
calico_typha_limits_memory: "{{ calico_typha_requests_memory }}"

# dns-aaaa-delay
dns_aaaa_delay_requests_cpu: "10m"
dns_aaaa_delay_requests_memory: "10Mi"
dns_aaaa_delay_limits_cpu: "{{ dns_aaaa_delay_requests_cpu }}"
dns_aaaa_delay_limits_memory: "{{ dns_aaaa_delay_requests_memory }}"

# default backend
default_backend_requests_cpu: "10m"
default_backend_requests_memory: "20Mi"
default_backend_limits_cpu: "{{ default_backend_requests_cpu }}"
default_backend_limits_memory: "{{ default_backend_requests_memory }}"

# heapster
heapster_requests_cpu: "150m"
heapster_requests_memory: "600Mi"
heapster_limits_cpu: "{{ heapster_requests_cpu }}"
heapster_limits_memory: "{{ heapster_requests_memory }}"

nanny_memory_kb_per_node: "200"
nanny_requests_cpu: "50m"
nanny_requests_memory: "{{ (groups['kube_cluster']|length * nanny_memory_kb_per_node|int) + (90 * 1024) }}Ki"
nanny_limits_cpu: "{{ nanny_requests_cpu }}"
nanny_limits_memory: "{{ nanny_requests_memory }}"
nanny_heapster_base_cpu: "{{ heapster_limits_cpu }}"
nanny_heapster_base_memory: "{{ heapster_limits_memory }}"
nanny_heapster_extra_cpu_per_node: "5m"
nanny_heapster_extra_memory_per_node: "4Mi"

heapster_legacy_requests_cpu: "200m"
heapster_legacy_requests_memory: "3Gi"
heapster_legacy_limits_cpu: "{{ heapster_legacy_requests_cpu }}"
heapster_legacy_limits_memory: "{{ heapster_legacy_requests_memory }}"

nanny_legacy_memory_kb_per_node: "200"
nanny_legacy_requests_cpu: "50m"
nanny_legacy_requests_memory: "{{ (groups['kube_cluster']|length * nanny_legacy_memory_kb_per_node|int) + (90 * 1024) }}Ki"
nanny_legacy_limits_cpu: "{{ nanny_legacy_requests_cpu }}"
nanny_legacy_limits_memory: "{{ nanny_legacy_requests_memory }}"
nanny_legacy_heapster_base_cpu: "{{ heapster_legacy_limits_cpu }}"
nanny_legacy_heapster_base_memory: "{{ heapster_legacy_limits_memory }}"
nanny_legacy_heapster_legacy_extra_cpu_per_node: "10m"
nanny_legacy_heapster_legacy_extra_memory_per_node: "16Mi"

# kube state metrics
kube_state_metrics_requests_cpu: "500m"  # the lower the cpu, the more likely things will get queued up which affects memory...also, gzip is enabled by default
kube_state_metrics_requests_memory: "600Mi"
kube_state_metrics_limits_cpu: "{{ kube_state_metrics_requests_cpu }}"
kube_state_metrics_limits_memory: "{{ kube_state_metrics_requests_memory }}"

kube_state_metrics_resizer_requests_cpu: "150m"
kube_state_metrics_resizer_requests_memory: "50Mi"
kube_state_metrics_resizer_limits_cpu: "{{ kube_state_metrics_resizer_requests_cpu }}"
kube_state_metrics_resizer_limits_memory: "{{ kube_state_metrics_resizer_requests_memory }}"

kube_state_metrics_resizer_setting_cpu: "{{ kube_state_metrics_requests_cpu }}"
kube_state_metrics_resizer_setting_extra_cpu: "10m"
kube_state_metrics_resizer_setting_memory: "{{ kube_state_metrics_requests_memory }}"
kube_state_metrics_resizer_setting_extra_memory: "20Mi"
kube_state_metrics_resizer_setting_threshold: "5"

# coredns
coredns_requests_cpu: "100m"
coredns_requests_memory: "200Mi"
coredns_limits_cpu: "{{ coredns_requests_cpu }}"
coredns_limits_memory: "{{ coredns_requests_memory }}"

# coredns autoscaler
coredns_autoscaler_requests_cpu: "30m"
coredns_autoscaler_requests_memory: "30Mi"
coredns_autoscaler_limits_cpu: "{{ coredns_autoscaler_requests_cpu }}"
coredns_autoscaler_limits_memory: "{{ coredns_autoscaler_requests_memory }}"

# kubernetes dashboard
kubernetes_dashboard_requests_cpu: "100m"
kubernetes_dashboard_requests_memory: "100Mi"
kubernetes_dashboard_limits_cpu: "{{ kubernetes_dashboard_requests_cpu }}"
kubernetes_dashboard_limits_memory: "{{ kubernetes_dashboard_requests_memory }}"

# nginx ingress controller
nginx_ingress_controller_requests_cpu: "100m"
nginx_ingress_controller_requests_memory: "200Mi"
nginx_ingress_controller_limits_cpu: "{{ nginx_ingress_controller_requests_cpu }}"
nginx_ingress_controller_limits_memory: "{{ nginx_ingress_controller_requests_memory }}"

# rook operator
rook_operator_requests_cpu: "200m"
rook_operator_requests_memory: "256Mi"
rook_operator_limits_cpu: "{{ rook_operator_requests_cpu }}"
rook_operator_limits_memory: "{{ rook_operator_requests_memory }}"
